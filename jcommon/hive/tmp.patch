diff --git a/jcommon/hive/CLAUDE.md b/jcommon/hive/CLAUDE.md
new file mode 100644
index 000000000..b5c31d8a8
--- /dev/null
+++ b/jcommon/hive/CLAUDE.md
@@ -0,0 +1,138 @@
+# CLAUDE.md
+
+This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.
+
+## Project Overview
+
+Hive is a Java-based AI agent framework that provides:
+- Agent-based architecture with roles (roles package)
+- Action execution system (actions package) 
+- Long-term memory capabilities (memory/longterm package)
+- MCP (Model Context Protocol) integration
+- Multi-LLM provider support
+
+The codebase is inspired by MetaGPT for agents, Spring AI and Cline for MCP implementation.
+
+## Development Commands
+
+### Build and Compile
+```bash
+mvn compile                    # Compile source code
+mvn test-compile              # Compile test code
+mvn clean compile             # Clean and compile
+```
+
+### Testing
+```bash
+mvn test                      # Run all tests
+mvn test -Dtest=ClassName     # Run specific test class
+mvn test -Dtest=ClassName#methodName  # Run specific test method
+```
+
+### IDE Configuration
+For IDE debugging, add this JVM parameter:
+```
+-Deditable.java.test.console=true
+```
+
+### Application Packaging and Running
+```bash
+mvn package                   # Build JAR
+java -jar target/app.jar      # Run the application
+```
+
+Kill running instances:
+```bash
+jps -l|grep app.jar|awk -F '' '{print $1}'|xargs kill -9
+```
+
+## Architecture Overview
+
+### Core Agent Framework
+- **Team-based architecture**: Multiple agents with different roles work together
+- **Think->Act cycle**: Main reasoning pattern for agents
+- **Role-Action-Team** pattern: Agents have roles, execute actions, and work in teams
+
+### Key Packages
+
+#### Roles (`src/main/java/run/mone/hive/roles/`)
+- `Architect`: System design and architecture planning
+- `Writer`: Documentation and writing tasks
+- `Teacher`: Educational content creation
+- `Human`: Human interaction handling
+- `DatabaseAssistant`: Database operations
+- `Coordinator`: Team coordination
+
+#### Actions (`src/main/java/run/mone/hive/actions/`)
+- Programmer actions: `WriteCode`, `FixBug`, `RunCode`, `WriteTest`, `DebugError`
+- Python-specific: `WritePythonCode`, `FixPythonBug`, `ExecutePythonCode`
+- Database actions: `QueryDataAction`, `ModifyDataAction`, `DesignSchemaAction`
+- Planning: `WritePlan`, `WriteDesign`, `UserRequirement`
+
+#### Long-term Memory (`src/main/java/run/mone/hive/memory/longterm/`)
+Java port of mem0 Python library with support for:
+- Multiple LLM providers (OpenAI, Claude, Gemini, Ollama)
+- Vector stores (Qdrant, Chroma, Weaviate, etc.)
+- Graph databases (Neo4j, Memgraph)
+- Multi-level memory (user, agent, session, procedural)
+
+#### MCP Integration (`src/main/java/run/mone/hive/mcp/`)
+Model Context Protocol implementation for tool integration:
+- Server and client implementations
+- Hub for managing multiple MCP connections
+- Transport layer abstraction
+
+### Configuration and Dependencies
+
+#### Key Dependencies
+- Java 17+ (compiled for 17, supports 21)
+- gRPC for communication
+- Jackson for JSON/YAML processing
+- Akka for actor model
+- OkHttp for HTTP clients
+- JUnit 5 + Mockito for testing
+
+#### Memory Module Dependencies
+Long-term memory module supports:
+- OpenAI GPT models
+- Anthropic Claude
+- Google Gemini
+- Ollama (local models)
+- Various vector databases
+- Graph databases (Neo4j, Memgraph)
+
+## Working with the Codebase
+
+### Memory System
+The long-term memory system is a key component:
+- Located in `src/main/java/run/mone/hive/memory/longterm/`
+- Configuration via `MemoryConfig`, `LlmConfig`, `EmbedderConfig`
+- Examples in `examples/` subdirectory
+- Comprehensive tests in `src/test/java/run/mone/hive/memory/longterm/`
+
+### Adding New Actions
+1. Extend `Action` base class
+2. Implement required methods
+3. Add to appropriate role's action list
+4. Register in `ActionFactory` if needed
+
+### Adding New Roles
+1. Create role class with appropriate actions
+2. Define role-specific prompts
+3. Integrate with team selection logic
+
+### Testing Strategy
+- Unit tests for individual components
+- Integration tests for memory system
+- Role-specific tests for agent behaviors
+- MCP integration tests
+
+## Project Structure Context
+
+This is part of the larger `mone` project at path `jcommon/hive`. The codebase follows Maven conventions with clear separation between:
+- Core framework (actions, roles, teams)
+- Memory systems (short-term, long-term)
+- Protocol integrations (MCP, gRPC)
+- Utilities and common components
+
+The project emphasizes modularity with factory patterns for LLM providers, vector stores, and embedding models to support easy extension and configuration.
\ No newline at end of file
diff --git a/jcommon/hive/pom.xml b/jcommon/hive/pom.xml
index a6a81f5a0..ec95985e7 100644
--- a/jcommon/hive/pom.xml
+++ b/jcommon/hive/pom.xml
@@ -185,6 +185,38 @@
             <version>1.5-jdk8-SNAPSHOT</version>
         </dependency>
 
+        <!-- Chroma Java client for embedded vector store -->
+        <dependency>
+            <groupId>io.github.amikos-tech</groupId>
+            <artifactId>chromadb-java-client</artifactId>
+            <version>0.1.7</version>
+            <optional>true</optional>
+        </dependency>
+
+        <!-- Qdrant Java client for vector store -->
+        <dependency>
+            <groupId>io.qdrant</groupId>
+            <artifactId>client</artifactId>
+            <version>1.15.0</version>
+            <optional>true</optional>
+        </dependency>
+
+        <!-- Kuzu Java client for embedded graph store -->
+        <dependency>
+            <groupId>com.kuzudb</groupId>
+            <artifactId>kuzu</artifactId>
+            <version>0.11.2</version>
+            <optional>true</optional>
+        </dependency>
+
+        <!-- SQLite for local embedded storage -->
+        <dependency>
+            <groupId>org.xerial</groupId>
+            <artifactId>sqlite-jdbc</artifactId>
+            <version>3.45.1.0</version>
+            <optional>true</optional>
+        </dependency>
+
     </dependencies>
 
     <build>
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/EmbedderConfig.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/EmbedderConfig.java
index 2713549ec..65b6104ef 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/EmbedderConfig.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/EmbedderConfig.java
@@ -84,6 +84,12 @@ public class EmbedderConfig {
      */
     @Builder.Default
     private Map<String, Object> config = new HashMap<>();
+
+    /**
+     * 自定义请求头
+     */
+    @Builder.Default
+    private Map<String, String> customHeaders = new HashMap<>();
     
     /**
      * 从Map创建配置
@@ -120,7 +126,13 @@ public class EmbedderConfig {
             Map<String, Object> config = (Map<String, Object>) configMap.get("config");
             builder.config(config);
         }
-        
+
+        if (configMap.containsKey("customHeaders")) {
+            @SuppressWarnings("unchecked")
+            Map<String, String> customHeaders = (Map<String, String>) configMap.get("customHeaders");
+            builder.customHeaders(customHeaders);
+        }
+
         return builder.build();
     }
     
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/GraphStoreConfig.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/GraphStoreConfig.java
index 948f9420b..e0026c078 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/GraphStoreConfig.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/GraphStoreConfig.java
@@ -84,6 +84,21 @@ public class GraphStoreConfig {
      */
     @Builder.Default
     private Map<String, Object> config = new HashMap<>();
+
+    /**
+     * LLM配置
+     */
+    private LlmConfig llm;
+
+    /**
+     * 自定义提示词
+     */
+    private String customPrompt;
+
+    /**
+     * @param configMap
+     * @return
+     */
     
     /**
      * 从Map创建配置
@@ -176,13 +191,24 @@ public class GraphStoreConfig {
     }
     
     /**
-     * 创建默认的Kuzu配置  
+     * 创建默认的Kuzu配置（本地嵌入式）
      */
     public static GraphStoreConfig kuzuDefault() {
         return GraphStoreConfig.builder()
             .provider(Provider.KUZU)
-            .url(":memory:")
-            .enabled(false)
+            .url("./data/kuzu")
+            .enabled(true)
+            .build();
+    }
+
+    /**
+     * 创建本地嵌入式Kuzu配置（测试用）
+     */
+    public static GraphStoreConfig kuzuEmbedded() {
+        return GraphStoreConfig.builder()
+            .provider(Provider.KUZU)
+            .url("./data/kuzu_embedded")
+            .enabled(true)
             .build();
     }
 }
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/LlmConfig.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/LlmConfig.java
index 4f4b5d0f9..ec9c1acd6 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/LlmConfig.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/LlmConfig.java
@@ -105,6 +105,12 @@ public class LlmConfig {
     @Builder.Default
     private String visionDetails = "low";
     
+    /**
+     * 自定义HTTP头
+     */
+    @Builder.Default
+    private Map<String, String> customHeaders = new HashMap<>();
+
     /**
      * 额外配置
      */
@@ -162,7 +168,13 @@ public class LlmConfig {
         if (configMap.containsKey("visionDetails")) {
             builder.visionDetails((String) configMap.get("visionDetails"));
         }
-        
+
+        if (configMap.containsKey("customHeaders")) {
+            @SuppressWarnings("unchecked")
+            Map<String, String> customHeaders = (Map<String, String>) configMap.get("customHeaders");
+            builder.customHeaders(customHeaders);
+        }
+
         if (configMap.containsKey("config")) {
             @SuppressWarnings("unchecked")
             Map<String, Object> config = (Map<String, Object>) configMap.get("config");
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/VectorStoreConfig.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/VectorStoreConfig.java
index 2da367649..834af319e 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/VectorStoreConfig.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/config/VectorStoreConfig.java
@@ -103,6 +103,16 @@ public class VectorStoreConfig {
      * 数据库名称 (对于支持数据库概念的存储)
      */
     private String database;
+
+    /**
+     * 嵌入函数名称
+     */
+    private String embeddingFunction;
+
+    /**
+     * 基础URL
+     */
+    private String baseUrl;
     
     /**
      * 从Map创建配置
@@ -148,7 +158,15 @@ public class VectorStoreConfig {
             Map<String, Object> config = (Map<String, Object>) configMap.get("config");
             builder.config(config);
         }
-        
+
+        if (configMap.containsKey("embeddingFunction")) {
+            builder.embeddingFunction((String) configMap.get("embeddingFunction"));
+        }
+
+        if (configMap.containsKey("baseUrl")) {
+            builder.baseUrl((String) configMap.get("baseUrl"));
+        }
+
         return builder.build();
     }
     
@@ -178,15 +196,29 @@ public class VectorStoreConfig {
     }
     
     /**
-     * 获取Chroma默认配置
+     * 获取Chroma默认配置（本地嵌入式）
      */
     public static VectorStoreConfig chromaDefault() {
         return VectorStoreConfig.builder()
                 .provider(Provider.CHROMA)
                 .host("localhost")
                 .port(8000)
+                .path("./data/chroma")
                 .collectionName("mem0")
                 .embeddingModelDims(1536)
                 .build();
     }
+
+    /**
+     * 获取本地嵌入式Chroma配置（测试用）
+     */
+    public static VectorStoreConfig chromaEmbedded() {
+        return VectorStoreConfig.builder()
+                .provider(Provider.CHROMA)
+                .host("localhost")
+                .path("./data/chroma_embedded")
+                .collectionName("embedded_mem0")
+                .embeddingModelDims(384)
+                .build();
+    }
 }
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/core/Memory.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/core/Memory.java
index 2bb3c148e..b5bc48644 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/core/Memory.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/core/Memory.java
@@ -404,7 +404,7 @@ public class Memory implements MemoryBase {
         List<String> facts = extractFacts(parsedMessages);
         
         if (facts.isEmpty()) {
-            log.debug("No facts extracted from input, skipping memory update");
+            log.info("No facts extracted from input, skipping memory update");
             return new ArrayList<>();
         }
         
@@ -539,7 +539,7 @@ public class Memory implements MemoryBase {
     }
     
     private String createMemory(String data, List<Double> existingEmbeddings, Map<String, Object> metadata) {
-        log.debug("Creating memory with data: {}", data);
+        log.info("Creating memory with data: {}", data);
         
         try {
             List<Double> embeddings = existingEmbeddings != null 
@@ -678,7 +678,7 @@ public class Memory implements MemoryBase {
                 }
             }
             
-            log.debug("Added {} relations to graph", relations.size());
+            log.info("Added {} relations to graph", relations.size());
             return relations;
             
         } catch (Exception e) {
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/embeddings/impl/HuggingFaceEmbedding.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/embeddings/impl/HuggingFaceEmbedding.java
index 938e73356..9f6de50b4 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/embeddings/impl/HuggingFaceEmbedding.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/embeddings/impl/HuggingFaceEmbedding.java
@@ -57,13 +57,19 @@ public class HuggingFaceEmbedding implements EmbeddingBase {
             String apiUrl = HF_API_URL + config.getModel();
             
             // 发送HTTP请求
-            HttpRequest httpRequest = HttpRequest.newBuilder()
+            HttpRequest.Builder requestBuilder = HttpRequest.newBuilder()
                 .uri(URI.create(apiUrl))
                 .header("Content-Type", "application/json")
                 .header("Authorization", "Bearer " + getApiKey())
                 .POST(HttpRequest.BodyPublishers.ofString(gson.toJson(request)))
-                .timeout(Duration.ofMinutes(2))
-                .build();
+                .timeout(Duration.ofMinutes(2));
+
+            // 添加自定义请求头
+            if (config.getCustomHeaders() != null) {
+                config.getCustomHeaders().forEach(requestBuilder::header);
+            }
+
+            HttpRequest httpRequest = requestBuilder.build();
             
             HttpResponse<String> response = httpClient.send(httpRequest, 
                 HttpResponse.BodyHandlers.ofString());
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/embeddings/impl/OllamaEmbedding.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/embeddings/impl/OllamaEmbedding.java
index 0e4b7a4fd..36fd1b30d 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/embeddings/impl/OllamaEmbedding.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/embeddings/impl/OllamaEmbedding.java
@@ -62,12 +62,18 @@ public class OllamaEmbedding implements EmbeddingBase {
             String apiUrl = config.getBaseUrl() + "/api/embeddings";
             
             // 发送HTTP请求
-            HttpRequest httpRequest = HttpRequest.newBuilder()
+            HttpRequest.Builder requestBuilder = HttpRequest.newBuilder()
                 .uri(URI.create(apiUrl))
                 .header("Content-Type", "application/json")
                 .POST(HttpRequest.BodyPublishers.ofString(gson.toJson(request)))
-                .timeout(Duration.ofMinutes(3))
-                .build();
+                .timeout(Duration.ofMinutes(3));
+
+            // 添加自定义请求头
+            if (config.getCustomHeaders() != null) {
+                config.getCustomHeaders().forEach(requestBuilder::header);
+            }
+
+            HttpRequest httpRequest = requestBuilder.build();
             
             HttpResponse<String> response = httpClient.send(httpRequest, 
                 HttpResponse.BodyHandlers.ofString());
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/embeddings/impl/OpenAiEmbedding.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/embeddings/impl/OpenAiEmbedding.java
index ef999f830..893e531c3 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/embeddings/impl/OpenAiEmbedding.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/embeddings/impl/OpenAiEmbedding.java
@@ -79,13 +79,19 @@ public class OpenAiEmbedding implements EmbeddingBase {
             String requestJson = gson.toJson(requestBody);
             
             // 发送请求
-            HttpRequest request = HttpRequest.newBuilder()
+            HttpRequest.Builder requestBuilder = HttpRequest.newBuilder()
                 .uri(URI.create(baseUrl + "/embeddings"))
                 .header("Content-Type", "application/json")
                 .header("Authorization", "Bearer " + apiKey)
                 .POST(HttpRequest.BodyPublishers.ofString(requestJson))
-                .timeout(Duration.ofMinutes(1))
-                .build();
+                .timeout(Duration.ofMinutes(1));
+
+            // 添加自定义请求头
+            if (config.getCustomHeaders() != null) {
+                config.getCustomHeaders().forEach(requestBuilder::header);
+            }
+
+            HttpRequest request = requestBuilder.build();
                 
             HttpResponse<String> response = httpClient.send(request, HttpResponse.BodyHandlers.ofString());
             
@@ -124,13 +130,19 @@ public class OpenAiEmbedding implements EmbeddingBase {
             String requestJson = gson.toJson(requestBody);
             
             // 发送请求
-            HttpRequest request = HttpRequest.newBuilder()
+            HttpRequest.Builder requestBuilder = HttpRequest.newBuilder()
                 .uri(URI.create(baseUrl + "/embeddings"))
                 .header("Content-Type", "application/json")
                 .header("Authorization", "Bearer " + apiKey)
                 .POST(HttpRequest.BodyPublishers.ofString(requestJson))
-                .timeout(Duration.ofMinutes(2))
-                .build();
+                .timeout(Duration.ofMinutes(2));
+
+            // 添加自定义请求头
+            if (config.getCustomHeaders() != null) {
+                config.getCustomHeaders().forEach(requestBuilder::header);
+            }
+
+            HttpRequest request = requestBuilder.build();
                 
             HttpResponse<String> response = httpClient.send(request, HttpResponse.BodyHandlers.ofString());
             
@@ -244,11 +256,17 @@ public class OpenAiEmbedding implements EmbeddingBase {
      */
     public List<String> getAvailableModels() {
         try {
-            HttpRequest request = HttpRequest.newBuilder()
+            HttpRequest.Builder requestBuilder = HttpRequest.newBuilder()
                 .uri(URI.create(baseUrl + "/models"))
                 .header("Authorization", "Bearer " + apiKey)
-                .GET()
-                .build();
+                .GET();
+
+            // 添加自定义请求头
+            if (config.getCustomHeaders() != null) {
+                config.getCustomHeaders().forEach(requestBuilder::header);
+            }
+
+            HttpRequest request = requestBuilder.build();
                 
             HttpResponse<String> response = httpClient.send(request, HttpResponse.BodyHandlers.ofString());
             
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/GraphStoreFactory.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/GraphStoreFactory.java
index 6a8cf66bd..c89a29e28 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/GraphStoreFactory.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/GraphStoreFactory.java
@@ -42,14 +42,27 @@ public class GraphStoreFactory {
     }
     
     /**
-     * 创建默认的Neo4j图数据库实例
-     * 
-     * @return Neo4j图数据库实例
+     * 创建默认的图数据库实例（优先本地嵌入式）
+     *
+     * @return 本地图数据库实例
      */
     public static GraphStoreBase createDefault() {
-        GraphStoreConfig config = GraphStoreConfig.neo4jDefault();
+        GraphStoreConfig config = GraphStoreConfig.kuzuDefault();
         return create(config);
     }
+
+    /**
+     * 为测试创建本地图数据库实例
+     *
+     * @return 本地Kuzu图数据库实例
+     */
+    public static GraphStoreBase createLocalForTesting() {
+        return create(GraphStoreConfig.builder()
+                .provider(GraphStoreConfig.Provider.KUZU)
+                .enabled(true)
+                .url("./data/test/kuzu")
+                .build());
+    }
     
     /**
      * 重置图数据库
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/impl/KuzuGraphStore.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/impl/KuzuGraphStore.java
index ade16b0c9..21617735b 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/impl/KuzuGraphStore.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/impl/KuzuGraphStore.java
@@ -4,67 +4,1186 @@ import lombok.extern.slf4j.Slf4j;
 import lombok.Data;
 import run.mone.hive.memory.longterm.config.GraphStoreConfig;
 import run.mone.hive.memory.longterm.graph.GraphStoreBase;
-import java.util.List;
-import java.util.Map;
-import java.util.ArrayList;
+
+import com.kuzudb.Connection;
+import com.kuzudb.Database;
+import com.kuzudb.QueryResult;
+
+import java.util.*;
+import java.util.stream.Collectors;
+import java.io.File;
+
+import run.mone.hive.memory.longterm.graph.tools.GraphTools;
+import run.mone.hive.memory.longterm.graph.utils.GraphUtils;
+import run.mone.hive.memory.longterm.llm.LLMBase;
+import run.mone.hive.memory.longterm.llm.LLMFactory;
 
 @Slf4j
 @Data
 public class KuzuGraphStore implements GraphStoreBase {
     private final GraphStoreConfig config;
-    
+    private Database database;
+    private Connection connection;
+    private boolean isEmbedded;
+    private String databasePath;
+
+    // Constants matching Python version
+    private static final String NODE_LABEL = ":Entity";
+    private static final String REL_LABEL = ":CONNECTED_TO";
+    private static final double DEFAULT_THRESHOLD = 0.7;
+
+    private LLMBase llm;
+
     public KuzuGraphStore(GraphStoreConfig config) {
         this.config = config;
-        log.info("Kuzu graph store initialized");
+        this.isEmbedded = initializeDatabase();
+        this.connection = createConnection();
+        this.llm = LLMFactory.create(config.getLlm());
+        setupSchema();
+
+        log.info("Kuzu graph store initialized {} at path: {}",
+                isEmbedded ? "in embedded mode" : "with remote connection",
+                databasePath);
+    }
+
+    private boolean initializeDatabase() {
+        try {
+            if (config.getUrl() == null 
+                || config.getUrl().isEmpty() 
+                || config.getUrl().trim().equalsIgnoreCase("")
+                || config.getUrl().trim().equalsIgnoreCase(":memory:")) {
+
+                log.info("Initializing embedded Kuzu database under in-memory mode");
+                database = new Database();
+                return true;
+
+            } else if (config.getUrl().startsWith("file://")
+                || config.getUrl().startsWith("file://")
+                || config.getUrl().startsWith("/") // for linux
+                || "local".equals(config.getUrl())) {
+
+                databasePath = config.getUrl() != null && config.getUrl().startsWith("file://")
+                    ? config.getUrl().substring(7)
+                    : (config.getUrl() != null && !"local".equals(config.getUrl())
+                        ? config.getUrl()
+                        : "kuzu-embedded");
+
+                File dbDir = new File(databasePath);
+                if (!dbDir.exists()) {
+                    dbDir.mkdirs();
+                    log.info("Created Kuzu database directory: {}", databasePath);
+                }
+
+                database = new Database(databasePath);
+                log.info("Initialized embedded Kuzu database at: {}", databasePath);
+                return true;
+
+            } else {
+                throw new UnsupportedOperationException("Remote Kuzu connections not yet supported");
+            }
+        } catch (Exception e) {
+            log.error("Failed to initialize Kuzu database: {}", e.getMessage());
+            throw new RuntimeException("Failed to initialize Kuzu database", e);
+        }
+    }
+
+    private Connection createConnection() {
+        try {
+            return new Connection(database);
+        } catch (Exception e) {
+            log.error("Failed to create Kuzu connection: {}", e.getMessage());
+            throw new RuntimeException("Failed to create Kuzu connection", e);
+        }
+    }
+
+    private void setupSchema() {
+        try {
+            // Create node table for entities matching Python version
+            executeQuery("""
+                CREATE NODE TABLE IF NOT EXISTS Entity(
+                    id SERIAL PRIMARY KEY,
+                    user_id STRING,
+                    agent_id STRING,
+                    run_id STRING,
+                    name STRING,
+                    mentions INT64,
+                    created TIMESTAMP,
+                    embedding FLOAT[]
+                )
+                """);
+
+            // Create relationship table matching Python version
+            executeQuery("""
+                CREATE REL TABLE IF NOT EXISTS CONNECTED_TO(
+                    FROM Entity TO Entity,
+                    name STRING,
+                    mentions INT64,
+                    created TIMESTAMP,
+                    updated TIMESTAMP
+                )
+                """);
+
+            log.info("Kuzu schema setup completed");
+        } catch (Exception e) {
+            log.error("Failed to setup Kuzu schema: {}", e.getMessage());
+            throw new RuntimeException("Failed to setup schema", e);
+        }
+    }
+
+    private QueryResult executeQuery(String query) {
+        try {
+            return connection.query(query);
+        } catch (Exception e) {
+            log.error("Failed to execute query: {} - Error: {}", query, e.getMessage());
+            throw new RuntimeException("Query execution failed", e);
+        }
+    }
+
+    /**
+     * Adds data to the graph matching Python version functionality.
+     *
+     * @param data The data to add to the graph
+     * @param filters A map containing filters (user_id, agent_id, run_id)
+     * @return Map containing deleted_entities and added_entities
+     */
+    public Map<String, Object> add(String data, Map<String, Object> filters) {
+        try {
+            // Extract entities from data (simplified - would use LLM in full implementation)
+            Map<String, String> entityTypeMap = retrieveNodesFromData(data, filters);
+
+            // Establish relations from data (simplified - would use LLM in full implementation)
+            List<Map<String, Object>> toBeAdded = establishNodesRelationsFromData(data, filters, entityTypeMap);
+
+            // Search graph database for existing nodes
+            List<Map<String, Object>> searchOutput = searchGraphDb(new ArrayList<>(entityTypeMap.keySet()), filters);
+
+            // Get entities to be deleted (simplified - would use LLM in full implementation)
+            List<Map<String, Object>> toBeDeleted = getDeleteEntitiesFromSearchOutput(searchOutput, data, filters);
+
+            // Delete old entities
+            List<Map<String, Object>> deletedEntities = deleteEntities(toBeDeleted, filters);
+
+            // Add new entities
+            List<Map<String, Object>> addedEntities = addEntities(toBeAdded, filters, entityTypeMap);
+
+            Map<String, Object> result = new HashMap<>();
+            result.put("deleted_entities", deletedEntities);
+            result.put("added_entities", addedEntities);
+            return result;
+        } catch (Exception e) {
+            log.error("Failed to add memory data: {}", e.getMessage());
+            Map<String, Object> result = new HashMap<>();
+            result.put("deleted_entities", new ArrayList<>());
+            result.put("added_entities", new ArrayList<>());
+            result.put("error", e.getMessage());
+            return result;
+        }
+    }
+
+    /**
+     * Search for memories and related graph data matching Python version.
+     *
+     * @param query Query to search for
+     * @param filters A map containing filters (user_id, agent_id, run_id)
+     * @param limit Maximum number of results to return
+     * @return List of search results with source, relationship, destination
+     */
+    public List<Map<String, Object>> search(String query, Map<String, Object> filters, int limit) {
+        try {
+            // Extract entities from query
+            Map<String, String> entityTypeMap = retrieveNodesFromData(query, filters);
+
+            // Search graph database
+            List<Map<String, Object>> searchOutput = searchGraphDb(new ArrayList<>(entityTypeMap.keySet()), filters);
+
+            if (searchOutput.isEmpty()) {
+                return new ArrayList<>();
+            }
+
+            // Simplified BM25 ranking - would use proper BM25 implementation in full version
+            List<Map<String, Object>> rankedResults = rankSearchResults(searchOutput, query, limit);
+
+            log.info("Returned {} search results", rankedResults.size());
+            return rankedResults;
+        } catch (Exception e) {
+            log.error("Failed to search graph: {}", e.getMessage());
+            return new ArrayList<>();
+        }
+    }
+
+    /**
+     * Delete all entities based on filters matching Python version.
+     *
+     * @param filters A map containing filters (user_id, agent_id, run_id)
+     */
+    public void deleteAll(Map<String, Object> filters) {
+        try {
+            List<String> nodeProps = new ArrayList<>();
+            nodeProps.add("user_id: $user_id");
+
+            Map<String, Object> params = new HashMap<>();
+            params.put("user_id", filters.get("user_id"));
+
+            if (filters.get("agent_id") != null) {
+                nodeProps.add("agent_id: $agent_id");
+                params.put("agent_id", filters.get("agent_id"));
+            }
+            if (filters.get("run_id") != null) {
+                nodeProps.add("run_id: $run_id");
+                params.put("run_id", filters.get("run_id"));
+            }
+
+            String nodePropsStr = String.join(", ", nodeProps);
+            String cypher = String.format("""
+                MATCH (n %s {%s})
+                DETACH DELETE n
+                """, NODE_LABEL, nodePropsStr);
+
+            executeQueryWithParams(cypher, params);
+            log.info("Deleted all entities with filters: {}", filters);
+        } catch (Exception e) {
+            log.error("Failed to delete all entities: {}", e.getMessage());
+            throw new RuntimeException("Failed to delete all entities", e);
+        }
+    }
+
+    /**
+     * Get all nodes and relationships based on filters matching Python version.
+     *
+     * @param filters A map containing filters (user_id, agent_id, run_id)
+     * @param limit Maximum number of results to return
+     * @return List of relationships with source, relationship, target
+     */
+    public List<Map<String, Object>> getAll(Map<String, Object> filters, int limit) {
+        try {
+            Map<String, Object> params = new HashMap<>();
+            params.put("user_id", filters.get("user_id"));
+            params.put("limit", limit);
+
+            List<String> nodeProps = new ArrayList<>();
+            nodeProps.add("user_id: $user_id");
+
+            if (filters.get("agent_id") != null) {
+                nodeProps.add("agent_id: $agent_id");
+                params.put("agent_id", filters.get("agent_id"));
+            }
+            if (filters.get("run_id") != null) {
+                nodeProps.add("run_id: $run_id");
+                params.put("run_id", filters.get("run_id"));
+            }
+
+            String nodePropsStr = String.join(", ", nodeProps);
+            String query = String.format("""
+                MATCH (n %s {%s})-[r]->(m %s {%s})
+                RETURN
+                    n.name AS source,
+                    r.name AS relationship,
+                    m.name AS target
+                LIMIT $limit
+                """, NODE_LABEL, nodePropsStr, NODE_LABEL, nodePropsStr);
+
+            List<Map<String, Object>> results = executeQueryWithParamsAndGetResults(query, params);
+            log.info("Retrieved {} relationships", results.size());
+            return results;
+        } catch (Exception e) {
+            log.error("Failed to get all relationships: {}", e.getMessage());
+            return new ArrayList<>();
+        }
     }
-    
+
     @Override
-    public Map<String, Object> addMemory(String source, String destination, String relationship, String sourceType, String destinationType) {
-        throw new UnsupportedOperationException("Kuzu implementation coming soon");
+    public Map<String, Object> addMemory(String source, String destination, String relationship,
+                                        String sourceType, String destinationType) {
+        try {
+            // Insert source entity if not exists
+            String sourceQuery = String.format("""
+                MERGE (s:Entity {name: '%s'})
+                ON CREATE SET s.type = '%s', s.created_at = timestamp()
+                """,
+                escapeString(source), escapeString(sourceType));
+            executeQuery(sourceQuery);
+
+            // Insert destination entity if not exists
+            String destQuery = String.format("""
+                MERGE (d:Entity {name: '%s'})
+                ON CREATE SET d.type = '%s', d.created_at = timestamp()
+                """,
+                escapeString(destination), escapeString(destinationType));
+            executeQuery(destQuery);
+
+            // Create relationship
+            String relQuery = String.format("""
+                MATCH (s:Entity {name: '%s'}), (d:Entity {name: '%s'})
+                CREATE (s)-[r:Relationship {type: '%s', created_at: timestamp(), updated_at: timestamp()}]->(d)
+                """,
+                escapeString(source), escapeString(destination), escapeString(relationship));
+            executeQuery(relQuery);
+
+            Map<String, Object> result = new HashMap<>();
+            result.put("action", "add_graph_memory");
+            result.put("source", source);
+            result.put("destination", destination);
+            result.put("relationship", relationship);
+            result.put("status", "success");
+
+            log.info("Added relationship: {} -[{}]-> {}", source, relationship, destination);
+            return result;
+        } catch (Exception e) {
+            log.error("Failed to add memory: {}", e.getMessage());
+            Map<String, Object> result = new HashMap<>();
+            result.put("action", "add_graph_memory");
+            result.put("status", "error");
+            result.put("error", e.getMessage());
+            return result;
+        }
     }
-    
+
     @Override
     public Map<String, Object> updateMemory(String source, String destination, String relationship) {
-        throw new UnsupportedOperationException("Kuzu implementation coming soon");
+        try {
+            String query = String.format("""
+                MATCH (s:Entity {name: '%s'})-[r:Relationship]->(d:Entity {name: '%s'})
+                SET r.type = '%s', r.updated_at = timestamp()
+                """,
+                escapeString(source), escapeString(destination), escapeString(relationship));
+            executeQuery(query);
+
+            Map<String, Object> result = new HashMap<>();
+            result.put("action", "update_graph_memory");
+            result.put("source", source);
+            result.put("destination", destination);
+            result.put("relationship", relationship);
+            result.put("status", "success");
+
+            log.info("Updated relationship: {} -[{}]-> {}", source, relationship, destination);
+            return result;
+        } catch (Exception e) {
+            log.error("Failed to update memory: {}", e.getMessage());
+            Map<String, Object> result = new HashMap<>();
+            result.put("action", "update_graph_memory");
+            result.put("status", "error");
+            result.put("error", e.getMessage());
+            return result;
+        }
     }
-    
+
     @Override
     public Map<String, Object> deleteMemory(String source, String destination, String relationship) {
-        throw new UnsupportedOperationException("Kuzu implementation coming soon");
+        try {
+            String query = String.format("""
+                MATCH (s:Entity {name: '%s'})-[r:Relationship {type: '%s'}]->(d:Entity {name: '%s'})
+                DELETE r
+                """,
+                escapeString(source), escapeString(relationship), escapeString(destination));
+            executeQuery(query);
+
+            Map<String, Object> result = new HashMap<>();
+            result.put("action", "delete_graph_memory");
+            result.put("source", source);
+            result.put("destination", destination);
+            result.put("relationship", relationship);
+            result.put("status", "success");
+
+            log.info("Deleted relationship: {} -[{}]-> {}", source, relationship, destination);
+            return result;
+        } catch (Exception e) {
+            log.error("Failed to delete memory: {}", e.getMessage());
+            Map<String, Object> result = new HashMap<>();
+            result.put("action", "delete_graph_memory");
+            result.put("status", "error");
+            result.put("error", e.getMessage());
+            return result;
+        }
     }
-    
+
     @Override
     public List<Map<String, Object>> search(String query, int limit) {
-        return new ArrayList<>();
+        try {
+            String searchQuery = String.format("""
+                MATCH (s:Entity)-[r:Relationship]->(d:Entity)
+                WHERE s.name CONTAINS '%s' OR d.name CONTAINS '%s' OR r.type CONTAINS '%s'
+                RETURN s.name as source, r.type as relationship, d.name as destination,
+                       s.type as source_type, d.type as destination_type
+                LIMIT %d
+                """,
+                escapeString(query), escapeString(query), escapeString(query), limit);
+
+            QueryResult result = executeQuery(searchQuery);
+            List<Map<String, Object>> results = new ArrayList<>();
+
+            while (result.hasNext()) {
+                var tuple = result.getNext();
+                Map<String, Object> row = new HashMap<>();
+                row.put("source", tuple.getValue(0).toString());
+                row.put("relationship", tuple.getValue(1).toString());
+                row.put("destination", tuple.getValue(2).toString());
+                row.put("source_type", tuple.getValue(3).toString());
+                row.put("destination_type", tuple.getValue(4).toString());
+                results.add(row);
+            }
+
+            log.info("Found {} relationships matching query: {}", results.size(), query);
+            return results;
+        } catch (Exception e) {
+            log.error("Failed to search memories: {}", e.getMessage());
+            return new ArrayList<>();
+        }
     }
-    
+
     @Override
     public List<Map<String, Object>> getAll(int limit) {
-        return new ArrayList<>();
+        try {
+            String query = String.format("""
+                MATCH (s:Entity)-[r:Relationship]->(d:Entity)
+                RETURN s.name as source, r.type as relationship, d.name as destination,
+                       s.type as source_type, d.type as destination_type
+                LIMIT %d
+                """, limit);
+
+            QueryResult result = executeQuery(query);
+            List<Map<String, Object>> results = new ArrayList<>();
+
+            while (result.hasNext()) {
+                var tuple = result.getNext();
+                Map<String, Object> row = new HashMap<>();
+                row.put("source", tuple.getValue(0).toString());
+                row.put("relationship", tuple.getValue(1).toString());
+                row.put("destination", tuple.getValue(2).toString());
+                row.put("source_type", tuple.getValue(3).toString());
+                row.put("destination_type", tuple.getValue(4).toString());
+                results.add(row);
+            }
+
+            return results;
+        } catch (Exception e) {
+            log.error("Failed to get all memories: {}", e.getMessage());
+            return new ArrayList<>();
+        }
     }
-    
+
     @Override
     public List<Map<String, Object>> extractEntities(String text) {
-        return new ArrayList<>();
+        // Simple entity extraction based on patterns
+        List<Map<String, Object>> entities = new ArrayList<>();
+
+        // This is a basic implementation - in real scenarios you'd use NLP libraries
+        String[] words = text.split("\\s+");
+        Set<String> uniqueEntities = new HashSet<>();
+
+        for (String word : words) {
+            word = word.replaceAll("[^a-zA-Z0-9]", "");
+            if (word.length() > 2 && Character.isUpperCase(word.charAt(0))) {
+                uniqueEntities.add(word);
+            }
+        }
+
+        for (String entity : uniqueEntities) {
+            Map<String, Object> entityMap = new HashMap<>();
+            entityMap.put("name", entity);
+            entityMap.put("type", "GENERAL");
+            entities.add(entityMap);
+        }
+
+        return entities;
     }
-    
+
     @Override
     public List<GraphEntity> establishRelations(String text) {
-        return new ArrayList<>();
+        // Create default filters for compatibility with existing interface
+        Map<String, Object> filters = new HashMap<>();
+        filters.put("user_id", "default_user");
+
+        // First extract entities
+        Map<String, String> entityTypeMap = retrieveNodesFromData(text, filters);
+
+        // Then establish relations using LLM
+        List<Map<String, Object>> relations = establishNodesRelationsFromData(text, filters, entityTypeMap);
+
+        // Convert to GraphEntity format
+        return relations.stream()
+                .map(rel -> new GraphEntity(
+                    rel.get("source").toString(),
+                    rel.get("destination").toString(),
+                    rel.get("relationship").toString(),
+                    "GENERAL",
+                    "GENERAL"
+                ))
+                .collect(Collectors.toList());
     }
-    
+
     @Override
     public boolean relationshipExists(String source, String destination, String relationship) {
-        return false;
+        try {
+            String query = String.format("""
+                MATCH (s:Entity {name: '%s'})-[r:Relationship {type: '%s'}]->(d:Entity {name: '%s'})
+                RETURN COUNT(r) as count
+                """,
+                escapeString(source), escapeString(relationship), escapeString(destination));
+
+            QueryResult result = executeQuery(query);
+            if (result.hasNext()) {
+                Long count = (Long) result.getNext().getValue(0).getValue();
+                return count > 0;
+            }
+            return false;
+        } catch (Exception e) {
+            log.error("Failed to check relationship existence: {}", e.getMessage());
+            return false;
+        }
     }
-    
+
     @Override
     public List<Map<String, Object>> getNodeRelationships(String nodeName) {
-        return new ArrayList<>();
+        try {
+            String query = String.format("""
+                MATCH (n:Entity {name: '%s'})-[r:Relationship]-(other:Entity)
+                RETURN CASE
+                         WHEN startNode(r) = n THEN 'outgoing'
+                         ELSE 'incoming'
+                       END as direction,
+                       r.type as relationship,
+                       other.name as other_node,
+                       other.type as other_type
+                """, escapeString(nodeName));
+
+            QueryResult result = executeQuery(query);
+            List<Map<String, Object>> relationships = new ArrayList<>();
+
+            while (result.hasNext()) {
+                var tuple = result.getNext();
+                Map<String, Object> rel = new HashMap<>();
+                rel.put("direction", tuple.getValue(0).toString());
+                rel.put("relationship", tuple.getValue(1).toString());
+                rel.put("other_node", tuple.getValue(2).toString());
+                rel.put("other_type", tuple.getValue(3).toString());
+                relationships.add(rel);
+            }
+
+            return relationships;
+        } catch (Exception e) {
+            log.error("Failed to get node relationships: {}", e.getMessage());
+            return new ArrayList<>();
+        }
     }
-    
+
     @Override
     public void deleteAll() {
-        throw new UnsupportedOperationException("Kuzu implementation coming soon");
+        try {
+            executeQuery("MATCH (n) DETACH DELETE n");
+            log.info("Deleted all data from Kuzu graph store");
+        } catch (Exception e) {
+            log.error("Failed to delete all data: {}", e.getMessage());
+            throw new RuntimeException("Failed to delete all data", e);
+        }
+    }
+
+    @Override
+    public boolean validateConnection() {
+        try {
+            executeQuery("MATCH (n) RETURN COUNT(n) LIMIT 1");
+            return true;
+        } catch (Exception e) {
+            return false;
+        }
+    }
+
+    @Override
+    public Map<String, Object> getStats() {
+        Map<String, Object> stats = new HashMap<>();
+        try {
+            // Get node count
+            QueryResult nodeResult = executeQuery("MATCH (n:Entity) RETURN COUNT(n) as count");
+            long nodeCount = 0;
+            if (nodeResult.hasNext()) {
+                nodeCount = (Long) nodeResult.getNext().getValue(0).getValue();
+            }
+
+            // Get relationship count
+            QueryResult relResult = executeQuery("MATCH ()-[r:Relationship]->() RETURN COUNT(r) as count");
+            long relCount = 0;
+            if (relResult.hasNext()) {
+                relCount = (Long) relResult.getNext().getValue(0).getValue();
+            }
+
+            stats.put("node_count", nodeCount);
+            stats.put("relationship_count", relCount);
+            stats.put("provider", "kuzu");
+            stats.put("embedded_mode", isEmbedded);
+            stats.put("database_path", databasePath);
+            stats.put("enabled", config.isEnabled());
+        } catch (Exception e) {
+            stats.put("node_count", 0);
+            stats.put("relationship_count", 0);
+            stats.put("error", e.getMessage());
+        }
+        return stats;
+    }
+
+    @Override
+    public void close() {
+        try {
+            if (connection != null) {
+                connection.close();
+            }
+            if (database != null) {
+                database.close();
+            }
+            log.info("Closed Kuzu graph store");
+        } catch (Exception e) {
+            log.warn("Error closing Kuzu connection: {}", e.getMessage());
+        }
+    }
+
+    private String escapeString(String input) {
+        if (input == null) return "";
+        return input.replace("'", "\\'").replace("\"", "\\\"");
+    }
+
+    // Helper methods to support the Python-equivalent functionality
+
+    /**
+     * Execute query with parameters (placeholder for proper parameter binding).
+     */
+    private void executeQueryWithParams(String query, Map<String, Object> params) {
+        // TODO: Implement proper parameter binding when KuzuDB Java API supports it
+        // For now, substitute parameters manually (not production-ready)
+        String processedQuery = substituteParameters(query, params);
+        executeQuery(processedQuery);
+    }
+
+    /**
+     * Execute query with parameters and return results.
+     */
+    private List<Map<String, Object>> executeQueryWithParamsAndGetResults(String query, Map<String, Object> params) {
+        // TODO: Implement proper parameter binding when KuzuDB Java API supports it
+        String processedQuery = substituteParameters(query, params);
+        QueryResult result = executeQuery(processedQuery);
+        List<Map<String, Object>> results = new ArrayList<>();
+
+        // Process results - this would need proper result parsing based on actual API
+        while (result.hasNext()) {
+            var tuple = result.getNext();
+            Map<String, Object> row = new HashMap<>();
+            // TODO: Map tuple values properly based on query structure
+            results.add(row);
+        }
+        return results;
+    }
+
+    /**
+     * Simple parameter substitution (not production-ready - use proper parameter binding).
+     */
+    private String substituteParameters(String query, Map<String, Object> params) {
+        String result = query;
+        for (Map.Entry<String, Object> entry : params.entrySet()) {
+            String placeholder = "$" + entry.getKey();
+            String value = entry.getValue() != null ? entry.getValue().toString() : "";
+            result = result.replace(placeholder, "'" + escapeString(value) + "'");
+        }
+        return result;
+    }
+
+    /**
+     * Extract entities from data using LLM.
+     * Matches Python _retrieve_nodes_from_data method.
+     */
+    private Map<String, String> retrieveNodesFromData(String data, Map<String, Object> filters) {
+        Map<String, String> entityTypeMap = new HashMap<>();
+
+        try {
+            // Build user identity for prompt
+            String userIdentity = GraphUtils.buildUserIdentity(filters);
+
+            // Prepare system prompt
+            String systemPrompt = GraphUtils.EXTRACT_ENTITIES_PROMPT.replace("USER_ID", userIdentity);
+
+            // Prepare messages for LLM
+            List<Map<String, Object>> messages = new ArrayList<>();
+            messages.add(Map.<String, Object>of("role", "system", "content", systemPrompt));
+            messages.add(Map.<String, Object>of("role", "user", "content", data));
+
+            // Prepare tools
+            List<Map<String, Object>> tools = new ArrayList<>();
+            tools.add(Map.<String, Object>of("tool", GraphTools.EXTRACT_ENTITIES_TOOL));
+
+            // Call LLM
+            Map<String, Object> response = llm.generateResponseWithTools(messages, tools);
+
+            // Parse response
+            if (response != null && response.get("tool_calls") != null) {
+                @SuppressWarnings("unchecked")
+                List<Map<String, Object>> toolCalls = (List<Map<String, Object>>) response.get("tool_calls");
+
+                for (Map<String, Object> toolCall : toolCalls) {
+                    if ("extract_entities".equals(toolCall.get("name"))) {
+                        @SuppressWarnings("unchecked")
+                        Map<String, Object> arguments = (Map<String, Object>) toolCall.get("arguments");
+
+                        if (arguments != null && arguments.get("entities") != null) {
+                            @SuppressWarnings("unchecked")
+                            List<Map<String, Object>> entities = (List<Map<String, Object>>) arguments.get("entities");
+
+                            for (Map<String, Object> entity : entities) {
+                                String entityName = entity.get("entity") != null ? entity.get("entity").toString() : "";
+                                String entityType = entity.get("entity_type") != null ? entity.get("entity_type").toString() : "general";
+
+                                if (!entityName.isEmpty()) {
+                                    entityTypeMap.put(
+                                        GraphUtils.normalizeEntityName(entityName),
+                                        GraphUtils.normalizeEntityName(entityType)
+                                    );
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        } catch (Exception e) {
+            log.error("Failed to extract entities using LLM: {}", e.getMessage());
+            // Fallback to simple extraction
+            String[] words = data.split("\\s+");
+            for (String word : words) {
+                word = word.replaceAll("[^a-zA-Z0-9]", "");
+                if (word.length() > 2 && Character.isUpperCase(word.charAt(0))) {
+                    entityTypeMap.put(GraphUtils.normalizeEntityName(word), "general");
+                }
+            }
+        }
+
+        log.debug("Entity type map: {}", entityTypeMap);
+        return entityTypeMap;
+    }
+
+    /**
+     * Establish relations from data using LLM.
+     * Matches Python _establish_nodes_relations_from_data method.
+     */
+    private List<Map<String, Object>> establishNodesRelationsFromData(String data, Map<String, Object> filters, Map<String, String> entityTypeMap) {
+        List<Map<String, Object>> entities = new ArrayList<>();
+
+        try {
+            // Build user identity for prompt
+            String userIdentity = GraphUtils.buildUserIdentity(filters);
+
+            // Check for custom prompt in config
+            String customPrompt = config.getCustomPrompt();
+
+            // Prepare system prompt
+            String systemPrompt;
+            List<Map<String, Object>> messages = new ArrayList<>();
+
+            if (customPrompt != null && !customPrompt.trim().isEmpty()) {
+                systemPrompt = GraphUtils.processPrompt(GraphUtils.EXTRACT_RELATIONS_PROMPT, userIdentity, customPrompt);
+                messages.add(Map.<String, Object>of("role", "system", "content", systemPrompt));
+                messages.add(Map.<String, Object>of("role", "user", "content", data));
+            } else {
+                systemPrompt = GraphUtils.processPrompt(GraphUtils.EXTRACT_RELATIONS_PROMPT, userIdentity, null);
+                messages.add(Map.<String, Object>of("role", "system", "content", systemPrompt));
+                String userContent = String.format("List of entities: %s. \n\nText: %s",
+                    new ArrayList<>(entityTypeMap.keySet()), data);
+                messages.add(Map.<String, Object>of("role", "user", "content", userContent));
+            }
+
+            // Prepare tools - would check LLM provider for structured tools
+            List<Map<String, Object>> tools = new ArrayList<>();
+            String llmProvider = config.getLlm() != null ? config.getLlm().getProvider().getValue() : "openai";
+
+            if ("azure_openai_structured".equals(llmProvider) || "openai_structured".equals(llmProvider)) {
+                tools.add(Map.<String, Object>of("tool", GraphTools.RELATIONS_STRUCT_TOOL));
+            } else {
+                tools.add(Map.<String, Object>of("tool", GraphTools.RELATIONS_TOOL));
+            }
+
+            // Call LLM
+            Map<String, Object> response = llm.generateResponseWithTools(messages, tools);
+
+            // Parse response
+            if (response != null && response.get("tool_calls") != null) {
+                @SuppressWarnings("unchecked")
+                List<Map<String, Object>> toolCalls = (List<Map<String, Object>>) response.get("tool_calls");
+
+                for (Map<String, Object> toolCall : toolCalls) {
+                    if ("establish_relationships".equals(toolCall.get("name"))) {
+                        @SuppressWarnings("unchecked")
+                        Map<String, Object> arguments = (Map<String, Object>) toolCall.get("arguments");
+
+                        if (arguments != null && arguments.get("entities") != null) {
+                            @SuppressWarnings("unchecked")
+                            List<Map<String, Object>> relationships = (List<Map<String, Object>>) arguments.get("entities");
+
+                            for (Map<String, Object> relationship : relationships) {
+                                Map<String, Object> normalizedRel = new HashMap<>();
+                                normalizedRel.put("source", GraphUtils.normalizeEntityName(
+                                    relationship.get("source") != null ? relationship.get("source").toString() : ""));
+                                normalizedRel.put("relationship", GraphUtils.normalizeEntityName(
+                                    relationship.get("relationship") != null ? relationship.get("relationship").toString() : ""));
+                                normalizedRel.put("destination", GraphUtils.normalizeEntityName(
+                                    relationship.get("destination") != null ? relationship.get("destination").toString() : ""));
+
+                                if (!normalizedRel.get("source").toString().isEmpty() &&
+                                    !normalizedRel.get("destination").toString().isEmpty() &&
+                                    !normalizedRel.get("relationship").toString().isEmpty()) {
+                                    entities.add(normalizedRel);
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        } catch (Exception e) {
+            log.error("Failed to extract relationships using LLM: {}", e.getMessage());
+            // Fallback to simple relationship extraction
+            List<String> entityList = new ArrayList<>(entityTypeMap.keySet());
+            if (entityList.size() >= 2) {
+                Map<String, Object> relation = new HashMap<>();
+                relation.put("source", entityList.get(0));
+                relation.put("destination", entityList.get(1));
+                relation.put("relationship", "related_to");
+                entities.add(relation);
+            }
+        }
+
+        log.debug("Extracted entities: {}", entities);
+        return entities;
+    }
+
+    /**
+     * Search graph database for similar nodes and their relationships.
+     */
+    private List<Map<String, Object>> searchGraphDb(List<String> nodeList, Map<String, Object> filters) {
+        List<Map<String, Object>> resultRelations = new ArrayList<>();
+
+        try {
+            Map<String, Object> params = new HashMap<>();
+            params.put("threshold", DEFAULT_THRESHOLD);
+            params.put("user_id", filters.get("user_id"));
+
+            List<String> nodeProps = new ArrayList<>();
+            nodeProps.add("user_id: $user_id");
+
+            if (filters.get("agent_id") != null) {
+                nodeProps.add("agent_id: $agent_id");
+                params.put("agent_id", filters.get("agent_id"));
+            }
+            if (filters.get("run_id") != null) {
+                nodeProps.add("run_id: $run_id");
+                params.put("run_id", filters.get("run_id"));
+            }
+
+            String nodePropsStr = String.join(", ", nodeProps);
+
+            for (String node : nodeList) {
+                // Generate embedding for the node (placeholder for actual embedding)
+                // In a full implementation, this would call an embedding model
+                double[] nodeEmbedding = generatePlaceholderEmbedding(node);
+
+                // Search using both text matching and embedding similarity
+                String query = String.format("""
+                    MATCH (n %s {%s})-[r]->(m %s {%s})
+                    WHERE n.name CONTAINS '%s' OR m.name CONTAINS '%s'
+                    RETURN
+                        n.name AS source,
+                        id(n) AS source_id,
+                        r.name AS relationship,
+                        id(r) AS relation_id,
+                        m.name AS destination,
+                        id(m) AS destination_id,
+                        1.0 AS similarity
+                    LIMIT 100
+                    """, NODE_LABEL, nodePropsStr, NODE_LABEL, nodePropsStr,
+                    escapeString(node), escapeString(node));
+
+                List<Map<String, Object>> results = executeQueryWithParamsAndGetResults(query, params);
+                resultRelations.addAll(results);
+            }
+        } catch (Exception e) {
+            log.error("Failed to search graph database: {}", e.getMessage());
+        }
+
+        return resultRelations;
+    }
+
+    /**
+     * Get entities to be deleted from search output using LLM.
+     * Matches Python _get_delete_entities_from_search_output method.
+     */
+    private List<Map<String, Object>> getDeleteEntitiesFromSearchOutput(List<Map<String, Object>> searchOutput, String data, Map<String, Object> filters) {
+        List<Map<String, Object>> toBeDeleted = new ArrayList<>();
+
+        try {
+            // Format search output
+            String searchOutputString = GraphUtils.formatEntities(searchOutput);
+
+            // Build user identity
+            String userIdentity = GraphUtils.buildUserIdentity(filters);
+
+            // Get delete messages
+            String[] messages = GraphUtils.getDeleteMessages(searchOutputString, data, userIdentity);
+            String systemPrompt = messages[0];
+            String userPrompt = messages[1];
+
+            // Prepare messages for LLM
+            List<Map<String, Object>> llmMessages = new ArrayList<>();
+            llmMessages.add(Map.<String, Object>of("role", "system", "content", systemPrompt));
+            llmMessages.add(Map.<String, Object>of("role", "user", "content", userPrompt));
+
+            // Prepare tools
+            String llmProvider = config.getLlm() != null ? config.getLlm().getProvider().getValue() : "openai";
+            List<Map<String, Object>> tools = new ArrayList<>();
+
+            if ("azure_openai_structured".equals(llmProvider) || "openai_structured".equals(llmProvider)) {
+                tools.add(Map.<String, Object>of("tool", GraphTools.DELETE_MEMORY_STRUCT_TOOL_GRAPH));
+            } else {
+                tools.add(Map.<String, Object>of("tool", GraphTools.DELETE_MEMORY_TOOL_GRAPH));
+            }
+
+            // Call LLM
+            Map<String, Object> response = llm.generateResponseWithTools(llmMessages, tools);
+
+            // Parse response
+            if (response != null && response.get("tool_calls") != null) {
+                @SuppressWarnings("unchecked")
+                List<Map<String, Object>> toolCalls = (List<Map<String, Object>>) response.get("tool_calls");
+
+                for (Map<String, Object> toolCall : toolCalls) {
+                    if ("delete_graph_memory".equals(toolCall.get("name"))) {
+                        @SuppressWarnings("unchecked")
+                        Map<String, Object> arguments = (Map<String, Object>) toolCall.get("arguments");
+
+                        if (arguments != null) {
+                            Map<String, Object> deleteItem = new HashMap<>();
+                            deleteItem.put("source", GraphUtils.normalizeEntityName(
+                                arguments.get("source") != null ? arguments.get("source").toString() : ""));
+                            deleteItem.put("relationship", GraphUtils.normalizeEntityName(
+                                arguments.get("relationship") != null ? arguments.get("relationship").toString() : ""));
+                            deleteItem.put("destination", GraphUtils.normalizeEntityName(
+                                arguments.get("destination") != null ? arguments.get("destination").toString() : ""));
+
+                            if (!deleteItem.get("source").toString().isEmpty() &&
+                                !deleteItem.get("destination").toString().isEmpty() &&
+                                !deleteItem.get("relationship").toString().isEmpty()) {
+                                toBeDeleted.add(deleteItem);
+                            }
+                        }
+                    }
+                }
+            }
+        } catch (Exception e) {
+            log.error("Failed to determine entities to delete using LLM: {}", e.getMessage());
+            // Fallback to simple logic
+            for (Map<String, Object> item : searchOutput) {
+                if (data.toLowerCase().contains("not") || data.toLowerCase().contains("no longer")) {
+                    toBeDeleted.add(item);
+                }
+            }
+        }
+
+        log.debug("Deleted relationships: {}", toBeDeleted);
+        return toBeDeleted;
+    }
+
+    /**
+     * Delete specific entities from the graph.
+     */
+    private List<Map<String, Object>> deleteEntities(List<Map<String, Object>> toBeDeleted, Map<String, Object> filters) {
+        List<Map<String, Object>> results = new ArrayList<>();
+
+        String userId = filters.get("user_id") != null ? filters.get("user_id").toString() : "";
+        String agentId = filters.get("agent_id") != null ? filters.get("agent_id").toString() : null;
+        String runId = filters.get("run_id") != null ? filters.get("run_id").toString() : null;
+
+        for (Map<String, Object> item : toBeDeleted) {
+            try {
+                String source = item.get("source") != null ? item.get("source").toString() : "";
+                String destination = item.get("destination") != null ? item.get("destination").toString() : "";
+                String relationship = item.get("relationship") != null ? item.get("relationship").toString() : "";
+
+                Map<String, Object> params = new HashMap<>();
+                params.put("source_name", source);
+                params.put("dest_name", destination);
+                params.put("user_id", userId);
+                params.put("relationship_name", relationship);
+
+                List<String> sourceProps = new ArrayList<>();
+                sourceProps.add("name: $source_name");
+                sourceProps.add("user_id: $user_id");
+
+                List<String> destProps = new ArrayList<>();
+                destProps.add("name: $dest_name");
+                destProps.add("user_id: $user_id");
+
+                if (agentId != null) {
+                    sourceProps.add("agent_id: $agent_id");
+                    destProps.add("agent_id: $agent_id");
+                    params.put("agent_id", agentId);
+                }
+                if (runId != null) {
+                    sourceProps.add("run_id: $run_id");
+                    destProps.add("run_id: $run_id");
+                    params.put("run_id", runId);
+                }
+
+                String sourcePropsStr = String.join(", ", sourceProps);
+                String destPropsStr = String.join(", ", destProps);
+
+                String cypher = String.format("""
+                    MATCH (n %s {%s})
+                    -[r %s {name: $relationship_name}]->
+                    (m %s {%s})
+                    DELETE r
+                    RETURN
+                        n.name AS source,
+                        r.name AS relationship,
+                        m.name AS target
+                    """, NODE_LABEL, sourcePropsStr, REL_LABEL, NODE_LABEL, destPropsStr);
+
+                List<Map<String, Object>> result = executeQueryWithParamsAndGetResults(cypher, params);
+                results.addAll(result);
+            } catch (Exception e) {
+                log.error("Failed to delete entity: {}", e.getMessage());
+            }
+        }
+
+        return results;
+    }
+
+    /**
+     * Add new entities to the graph.
+     */
+    private List<Map<String, Object>> addEntities(List<Map<String, Object>> toBeAdded, Map<String, Object> filters, Map<String, String> entityTypeMap) {
+        List<Map<String, Object>> results = new ArrayList<>();
+
+        String userId = filters.get("user_id") != null ? filters.get("user_id").toString() : "";
+        String agentId = filters.get("agent_id") != null ? filters.get("agent_id").toString() : null;
+        String runId = filters.get("run_id") != null ? filters.get("run_id").toString() : null;
+
+        for (Map<String, Object> item : toBeAdded) {
+            try {
+                String source = item.get("source") != null ? item.get("source").toString() : "";
+                String destination = item.get("destination") != null ? item.get("destination").toString() : "";
+                String relationship = item.get("relationship") != null ? item.get("relationship").toString() : "";
+
+                // TODO: Add embedding generation when embedding support is available
+                // For now, create entities without embeddings
+
+                Map<String, Object> params = new HashMap<>();
+                params.put("source_name", source);
+                params.put("dest_name", destination);
+                params.put("relationship_name", relationship);
+                params.put("user_id", userId);
+
+                List<String> sourceProps = new ArrayList<>();
+                sourceProps.add("name: $source_name");
+                sourceProps.add("user_id: $user_id");
+
+                List<String> destProps = new ArrayList<>();
+                destProps.add("name: $dest_name");
+                destProps.add("user_id: $user_id");
+
+                if (agentId != null) {
+                    sourceProps.add("agent_id: $agent_id");
+                    destProps.add("agent_id: $agent_id");
+                    params.put("agent_id", agentId);
+                }
+                if (runId != null) {
+                    sourceProps.add("run_id: $run_id");
+                    destProps.add("run_id: $run_id");
+                    params.put("run_id", runId);
+                }
+
+                String sourcePropsStr = String.join(", ", sourceProps);
+                String destPropsStr = String.join(", ", destProps);
+
+                String cypher = String.format("""
+                    MERGE (source %s {%s})
+                    ON CREATE SET
+                        source.created = current_timestamp(),
+                        source.mentions = 1
+                    ON MATCH SET
+                        source.mentions = coalesce(source.mentions, 0) + 1
+                    WITH source
+                    MERGE (destination %s {%s})
+                    ON CREATE SET
+                        destination.created = current_timestamp(),
+                        destination.mentions = 1
+                    ON MATCH SET
+                        destination.mentions = coalesce(destination.mentions, 0) + 1
+                    WITH source, destination
+                    MERGE (source)-[rel %s {name: $relationship_name}]->(destination)
+                    ON CREATE SET
+                        rel.created = current_timestamp(),
+                        rel.mentions = 1
+                    ON MATCH SET
+                        rel.mentions = coalesce(rel.mentions, 0) + 1
+                    RETURN
+                        source.name AS source,
+                        rel.name AS relationship,
+                        destination.name AS target
+                    """, NODE_LABEL, sourcePropsStr, NODE_LABEL, destPropsStr, REL_LABEL);
+
+                List<Map<String, Object>> result = executeQueryWithParamsAndGetResults(cypher, params);
+                results.addAll(result);
+            } catch (Exception e) {
+                log.error("Failed to add entity: {}", e.getMessage());
+            }
+        }
+
+        return results;
+    }
+
+    /**
+     * Rank search results using simplified BM25-like scoring.
+     */
+    private List<Map<String, Object>> rankSearchResults(List<Map<String, Object>> searchOutput, String query, int limit) {
+        // Simplified ranking - would use proper BM25 implementation
+        List<String> queryTokens = Arrays.asList(query.toLowerCase().split("\\s+"));
+
+        return searchOutput.stream()
+                .sorted((a, b) -> {
+                    double scoreA = calculateRelevanceScore(a, queryTokens);
+                    double scoreB = calculateRelevanceScore(b, queryTokens);
+                    return Double.compare(scoreB, scoreA);
+                })
+                .limit(limit)
+                .map(item -> {
+                    Map<String, Object> result = new HashMap<>();
+                    result.put("source", item.get("source"));
+                    result.put("relationship", item.get("relationship"));
+                    result.put("destination", item.get("destination"));
+                    return result;
+                })
+                .collect(Collectors.toList());
+    }
+
+    /**
+     * Calculate relevance score for ranking.
+     */
+    private double calculateRelevanceScore(Map<String, Object> item, List<String> queryTokens) {
+        double score = 0.0;
+        String source = item.get("source") != null ? item.get("source").toString().toLowerCase() : "";
+        String relationship = item.get("relationship") != null ? item.get("relationship").toString().toLowerCase() : "";
+        String destination = item.get("destination") != null ? item.get("destination").toString().toLowerCase() : "";
+
+        for (String token : queryTokens) {
+            if (source.contains(token)) score += 1.0;
+            if (relationship.contains(token)) score += 0.5;
+            if (destination.contains(token)) score += 1.0;
+        }
+
+        return score;
+    }
+
+    /**
+     * Generate placeholder embedding for a text.
+     * This should be replaced with actual embedding model when available.
+     */
+    private double[] generatePlaceholderEmbedding(String text) {
+        // Simple hash-based placeholder embedding
+        int hash = text.hashCode();
+        double[] embedding = new double[384]; // Common embedding dimension
+        for (int i = 0; i < embedding.length; i++) {
+            embedding[i] = Math.sin(hash * (i + 1) * 0.001);
+        }
+        return embedding;
     }
-}
+}
\ No newline at end of file
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/tools/GraphTools.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/tools/GraphTools.java
new file mode 100644
index 000000000..42b77d4c4
--- /dev/null
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/tools/GraphTools.java
@@ -0,0 +1,210 @@
+package run.mone.hive.memory.longterm.graph.tools;
+
+/**
+ * Graph tools and tool definitions matching Python mem0 implementation.
+ * Corresponds to mem0/graphs/tools.py
+ */
+public class GraphTools {
+
+    /**
+     * Tool for establishing relationships between entities.
+     * Matches Python RELATIONS_TOOL
+     */
+    public static final String RELATIONS_TOOL = """
+            {
+                "type": "function",
+                "function": {
+                    "name": "establish_relationships",
+                    "description": "Establish relationships among the entities based on the provided text.",
+                    "parameters": {
+                        "type": "object",
+                        "properties": {
+                            "entities": {
+                                "type": "array",
+                                "items": {
+                                    "type": "object",
+                                    "properties": {
+                                        "source": {"type": "string", "description": "The source entity of the relationship."},
+                                        "relationship": {"type": "string", "description": "The relationship between the source and destination entities."},
+                                        "destination": {"type": "string", "description": "The destination entity of the relationship."}
+                                    },
+                                    "required": ["source", "relationship", "destination"],
+                                    "additionalProperties": false
+                                }
+                            }
+                        },
+                        "required": ["entities"],
+                        "additionalProperties": false
+                    }
+                }
+            }
+            """;
+
+    /**
+     * Structured version of RELATIONS_TOOL for structured LLM providers.
+     * Matches Python RELATIONS_STRUCT_TOOL
+     */
+    public static final String RELATIONS_STRUCT_TOOL = """
+            {
+                "type": "function",
+                "function": {
+                    "name": "establish_relationships",
+                    "description": "Establish relationships among the entities based on the provided text.",
+                    "parameters": {
+                        "type": "object",
+                        "properties": {
+                            "entities": {
+                                "type": "array",
+                                "items": {
+                                    "type": "object",
+                                    "properties": {
+                                        "source": {"type": "string", "description": "The source entity of the relationship."},
+                                        "relationship": {"type": "string", "description": "The relationship between the source and destination entities."},
+                                        "destination": {"type": "string", "description": "The destination entity of the relationship."}
+                                    },
+                                    "required": ["source", "relationship", "destination"],
+                                    "additionalProperties": false
+                                }
+                            }
+                        },
+                        "required": ["entities"],
+                        "additionalProperties": false
+                    }
+                }
+            }
+            """;
+
+    /**
+     * Tool for extracting entities from text.
+     * Matches Python EXTRACT_ENTITIES_TOOL
+     */
+    public static final String EXTRACT_ENTITIES_TOOL = """
+            {
+                "type": "function",
+                "function": {
+                    "name": "extract_entities",
+                    "description": "Extract entities and their types from the given text.",
+                    "parameters": {
+                        "type": "object",
+                        "properties": {
+                            "entities": {
+                                "type": "array",
+                                "items": {
+                                    "type": "object",
+                                    "properties": {
+                                        "entity": {"type": "string", "description": "The entity name."},
+                                        "entity_type": {"type": "string", "description": "The type of the entity."}
+                                    },
+                                    "required": ["entity", "entity_type"],
+                                    "additionalProperties": false
+                                }
+                            }
+                        },
+                        "required": ["entities"],
+                        "additionalProperties": false
+                    }
+                }
+            }
+            """;
+
+    /**
+     * Structured version of EXTRACT_ENTITIES_TOOL for structured LLM providers.
+     * Matches Python EXTRACT_ENTITIES_STRUCT_TOOL
+     */
+    public static final String EXTRACT_ENTITIES_STRUCT_TOOL = """
+            {
+                "type": "function",
+                "function": {
+                    "name": "extract_entities",
+                    "description": "Extract entities and their types from the given text.",
+                    "parameters": {
+                        "type": "object",
+                        "properties": {
+                            "entities": {
+                                "type": "array",
+                                "items": {
+                                    "type": "object",
+                                    "properties": {
+                                        "entity": {"type": "string", "description": "The entity name."},
+                                        "entity_type": {"type": "string", "description": "The type of the entity."}
+                                    },
+                                    "required": ["entity", "entity_type"],
+                                    "additionalProperties": false
+                                }
+                            }
+                        },
+                        "required": ["entities"],
+                        "additionalProperties": false
+                    }
+                }
+            }
+            """;
+
+    /**
+     * Tool for deleting graph memory.
+     * Matches Python DELETE_MEMORY_TOOL_GRAPH
+     */
+    public static final String DELETE_MEMORY_TOOL_GRAPH = """
+            {
+                "type": "function",
+                "function": {
+                    "name": "delete_graph_memory",
+                    "description": "Delete a specific relationship from the graph memory.",
+                    "parameters": {
+                        "type": "object",
+                        "properties": {
+                            "source": {"type": "string", "description": "The source entity of the relationship to delete."},
+                            "relationship": {"type": "string", "description": "The relationship type to delete."},
+                            "destination": {"type": "string", "description": "The destination entity of the relationship to delete."}
+                        },
+                        "required": ["source", "relationship", "destination"],
+                        "additionalProperties": false
+                    }
+                }
+            }
+            """;
+
+    /**
+     * Structured version of DELETE_MEMORY_TOOL_GRAPH for structured LLM providers.
+     * Matches Python DELETE_MEMORY_STRUCT_TOOL_GRAPH
+     */
+    public static final String DELETE_MEMORY_STRUCT_TOOL_GRAPH = """
+            {
+                "type": "function",
+                "function": {
+                    "name": "delete_graph_memory",
+                    "description": "Delete a specific relationship from the graph memory.",
+                    "parameters": {
+                        "type": "object",
+                        "properties": {
+                            "source": {"type": "string", "description": "The source entity of the relationship to delete."},
+                            "relationship": {"type": "string", "description": "The relationship type to delete."},
+                            "destination": {"type": "string", "description": "The destination entity of the relationship to delete."}
+                        },
+                        "required": ["source", "relationship", "destination"],
+                        "additionalProperties": false
+                    }
+                }
+            }
+            """;
+
+    /**
+     * No-operation tool for when no graph changes are needed.
+     * Matches Python NOOP_TOOL
+     */
+    public static final String NOOP_TOOL = """
+            {
+                "type": "function",
+                "function": {
+                    "name": "noop",
+                    "description": "No operation should be performed to the graph entities. This function is called when the system determines that no changes or additions are necessary based on the current input or context.",
+                    "parameters": {
+                        "type": "object",
+                        "properties": {},
+                        "required": [],
+                        "additionalProperties": false
+                    }
+                }
+            }
+            """;
+}
\ No newline at end of file
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/utils/GraphUtils.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/utils/GraphUtils.java
new file mode 100644
index 000000000..cb1f32223
--- /dev/null
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/graph/utils/GraphUtils.java
@@ -0,0 +1,208 @@
+package run.mone.hive.memory.longterm.graph.utils;
+
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Graph utilities and prompts matching Python mem0 implementation.
+ * Corresponds to mem0/graphs/utils.py
+ */
+public class GraphUtils {
+
+    /**
+     * Prompt for extracting relationships between entities.
+     * Matches Python EXTRACT_RELATIONS_PROMPT
+     */
+    public static final String EXTRACT_RELATIONS_PROMPT = """
+
+            You are an advanced algorithm designed to extract structured information from text to construct knowledge graphs. Your goal is to capture comprehensive and accurate information. Follow these key principles:
+
+            1. Extract only explicitly stated information from the text.
+            2. Establish relationships among the entities provided.
+            3. Use "USER_ID" as the source entity for any self-references (e.g., "I," "me," "my," etc.) in user messages.
+            CUSTOM_PROMPT
+
+            Relationships:
+            - Identify clear, explicit relationships between entities
+            - Use descriptive relationship types (e.g., WORKS_AT, LIVES_IN, KNOWS, etc.)
+            - Ensure relationships are bidirectional where appropriate
+            - Focus on factual, observable connections
+
+            Entity Guidelines:
+            - Use the exact entity names provided in the entity list
+            - Normalize entity names (lowercase, underscore-separated)
+            - Map self-references to USER_ID
+            """;
+
+    /**
+     * Prompt for extracting entities from text.
+     * Used in entity extraction phase.
+     */
+    public static final String EXTRACT_ENTITIES_PROMPT = """
+            You are a smart assistant who understands entities and their types in a given text.
+            If user message contains self reference such as 'I', 'me', 'my' etc. then use USER_ID as the source entity.
+            Extract all the entities from the text. ***DO NOT*** answer the question itself if the given text is a question.
+            """;
+
+    /**
+     * Prompt for updating existing graph memories.
+     * Matches Python UPDATE_MEMORY_PROMPT
+     */
+    public static final String UPDATE_MEMORY_PROMPT = """
+            You are a memory update specialist for a graph-based memory system. Your task is to analyze existing graph memories and update them based on new information provided. You will receive:
+
+            1. Existing Graph Memories: A list of current graph memories, each containing source, target, and relationship information.
+            2. New Graph Memory: Fresh information to be integrated into the existing graph structure.
+
+            Guidelines:
+            1. Identification: Use the source and target as primary identifiers when matching existing memories with new information.
+            2. Conflict Resolution:
+               - If new information contradicts an existing memory:
+                 a) For matching source and target but differing content, update the relationship of the existing memory.
+                 b) If the new memory provides more recent or accurate information, update the existing memory accordingly.
+            3. Comprehensive Review: Thoroughly examine each existing graph memory against the new information, updating relationships as necessary. Multiple updates may be required.
+            4. Consistency: Maintain a uniform and clear style across all memories. Each entry should be concise yet comprehensive.
+            5. Semantic Coherence: Ensure that updates maintain or improve the overall semantic structure of the graph.
+            6. Temporal Awareness: If timestamps are available, consider the recency of information when making updates.
+            7. Relationship Refinement: Look for opportunities to refine relationship descriptions for greater precision or clarity.
+            8. Redundancy Elimination: Identify and merge any redundant or highly similar relationships that may result from the update.
+
+            Memory Format:
+            source -- RELATIONSHIP -- destination
+
+            Task Details:
+            ======= Existing Graph Memories:=======
+            {existing_memories}
+
+            ======= New Graph Memory:=======
+            {new_memories}
+
+            Output:
+            Provide a list of update instructions, each specifying the source, target, and the new relationship to be set. Only include memories that require updates.
+            """;
+
+    /**
+     * Format entities for display or processing.
+     * Matches Python format_entities function
+     *
+     * @param entities List of entity maps with source, relationship, destination
+     * @return Formatted string representation
+     */
+    public static String formatEntities(List<Map<String, Object>> entities) {
+        if (entities == null || entities.isEmpty()) {
+            return "";
+        }
+
+        StringBuilder formatted = new StringBuilder();
+        for (Map<String, Object> entity : entities) {
+            String source = entity.get("source") != null ? entity.get("source").toString() : "";
+            String relationship = entity.get("relationship") != null ? entity.get("relationship").toString() : "";
+            String destination = entity.get("destination") != null ? entity.get("destination").toString() : "";
+
+            formatted.append(source)
+                    .append(" -- ")
+                    .append(relationship)
+                    .append(" -- ")
+                    .append(destination)
+                    .append("\n");
+        }
+
+        return formatted.toString().trim();
+    }
+
+    /**
+     * Get delete messages for LLM prompting.
+     * Matches Python get_delete_messages function
+     *
+     * @param searchOutputString Formatted search output
+     * @param data New data to compare against
+     * @param userIdentity User identification string
+     * @return Array containing [system_prompt, user_prompt]
+     */
+    public static String[] getDeleteMessages(String searchOutputString, String data, String userIdentity) {
+        String systemPrompt = """
+                You are a memory deletion specialist for a graph-based memory system. Your task is to identify which existing graph relationships should be deleted based on new information.
+
+                Analyze the existing relationships and determine if any should be removed because:
+                1. The new information contradicts them
+                2. The new information makes them obsolete
+                3. The new information provides more accurate alternatives
+
+                User Identity: """ + userIdentity + """
+
+                Guidelines:
+                - Only mark relationships for deletion if there's clear evidence they are incorrect or obsolete
+                - Be conservative - when in doubt, don't delete
+                - Consider temporal aspects - newer information may supersede older information
+                - Focus on factual contradictions, not subjective differences
+                """;
+
+        String userPrompt = String.format("""
+                Existing relationships:
+                %s
+
+                New information:
+                %s
+
+                Identify which existing relationships should be deleted based on the new information.
+                """, searchOutputString, data);
+
+        return new String[]{systemPrompt, userPrompt};
+    }
+
+    /**
+     * Normalize entity names by converting to lowercase and replacing spaces with underscores.
+     * Matches Python entity normalization logic
+     *
+     * @param entityName The entity name to normalize
+     * @return Normalized entity name
+     */
+    public static String normalizeEntityName(String entityName) {
+        if (entityName == null) {
+            return "";
+        }
+        return entityName.toLowerCase().replace(" ", "_");
+    }
+
+    /**
+     * Replace placeholders in prompts with actual values.
+     *
+     * @param prompt The prompt template
+     * @param userIdentity User identification string
+     * @param customPrompt Custom prompt to insert (can be null)
+     * @return Processed prompt with replacements
+     */
+    public static String processPrompt(String prompt, String userIdentity, String customPrompt) {
+        String processed = prompt.replace("USER_ID", userIdentity);
+
+        if (customPrompt != null && !customPrompt.trim().isEmpty()) {
+            processed = processed.replace("CUSTOM_PROMPT", "4. " + customPrompt);
+        } else {
+            processed = processed.replace("CUSTOM_PROMPT", "");
+        }
+
+        return processed;
+    }
+
+    /**
+     * Build user identity string from filters.
+     * Matches Python user identity composition logic
+     *
+     * @param filters Map containing user_id, agent_id, run_id
+     * @return Formatted user identity string
+     */
+    public static String buildUserIdentity(Map<String, Object> filters) {
+        StringBuilder identity = new StringBuilder();
+        identity.append("user_id: ").append(filters.get("user_id"));
+
+        if (filters.get("agent_id") != null) {
+            identity.append(", agent_id: ").append(filters.get("agent_id"));
+        }
+
+        if (filters.get("run_id") != null) {
+            identity.append(", run_id: ").append(filters.get("run_id"));
+        }
+
+        return identity.toString();
+    }
+}
\ No newline at end of file
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/ClaudeLLM.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/ClaudeLLM.java
index c59e48ed4..032c004ac 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/ClaudeLLM.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/ClaudeLLM.java
@@ -55,14 +55,20 @@ public class ClaudeLLM implements LLMBase {
             JsonObject request = buildRequest(messages);
             
             // 发送HTTP请求
-            HttpRequest httpRequest = HttpRequest.newBuilder()
+            HttpRequest.Builder requestBuilder = HttpRequest.newBuilder()
                 .uri(URI.create(ANTHROPIC_API_URL))
                 .header("Content-Type", "application/json")
                 .header("anthropic-version", "2023-06-01")
                 .header("x-api-key", getApiKey())
                 .POST(HttpRequest.BodyPublishers.ofString(gson.toJson(request)))
-                .timeout(Duration.ofMinutes(2))
-                .build();
+                .timeout(Duration.ofMinutes(2));
+
+            // 添加自定义头
+            if (config.getCustomHeaders() != null) {
+                config.getCustomHeaders().forEach(requestBuilder::header);
+            }
+
+            HttpRequest httpRequest = requestBuilder.build();
             
             HttpResponse<String> response = httpClient.send(httpRequest, 
                 HttpResponse.BodyHandlers.ofString());
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/GeminiLLM.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/GeminiLLM.java
index 5f79af9db..2f5e00de8 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/GeminiLLM.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/GeminiLLM.java
@@ -57,12 +57,18 @@ public class GeminiLLM implements LLMBase {
             String apiUrl = String.format(GEMINI_API_URL, config.getModel()) + "?key=" + getApiKey();
             
             // 发送HTTP请求
-            HttpRequest httpRequest = HttpRequest.newBuilder()
+            HttpRequest.Builder requestBuilder = HttpRequest.newBuilder()
                 .uri(URI.create(apiUrl))
                 .header("Content-Type", "application/json")
                 .POST(HttpRequest.BodyPublishers.ofString(gson.toJson(request)))
-                .timeout(Duration.ofMinutes(2))
-                .build();
+                .timeout(Duration.ofMinutes(2));
+
+            // 添加自定义头
+            if (config.getCustomHeaders() != null) {
+                config.getCustomHeaders().forEach(requestBuilder::header);
+            }
+
+            HttpRequest httpRequest = requestBuilder.build();
             
             HttpResponse<String> response = httpClient.send(httpRequest, 
                 HttpResponse.BodyHandlers.ofString());
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/OllamaLLM.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/OllamaLLM.java
index 32bf27d04..80a5be967 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/OllamaLLM.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/OllamaLLM.java
@@ -61,12 +61,18 @@ public class OllamaLLM implements LLMBase {
             String apiUrl = config.getBaseUrl() + "/api/chat";
             
             // 发送HTTP请求
-            HttpRequest httpRequest = HttpRequest.newBuilder()
+            HttpRequest.Builder requestBuilder = HttpRequest.newBuilder()
                 .uri(URI.create(apiUrl))
                 .header("Content-Type", "application/json")
                 .POST(HttpRequest.BodyPublishers.ofString(gson.toJson(request)))
-                .timeout(Duration.ofMinutes(5)) // Ollama可能比较慢
-                .build();
+                .timeout(Duration.ofMinutes(5)); // Ollama可能比较慢
+
+            // 添加自定义头
+            if (config.getCustomHeaders() != null) {
+                config.getCustomHeaders().forEach(requestBuilder::header);
+            }
+
+            HttpRequest httpRequest = requestBuilder.build();
             
             HttpResponse<String> response = httpClient.send(httpRequest, 
                 HttpResponse.BodyHandlers.ofString());
@@ -168,11 +174,17 @@ public class OllamaLLM implements LLMBase {
         try {
             String apiUrl = config.getBaseUrl() + "/api/tags";
             
-            HttpRequest request = HttpRequest.newBuilder()
+            HttpRequest.Builder requestBuilder = HttpRequest.newBuilder()
                 .uri(URI.create(apiUrl))
                 .GET()
-                .timeout(Duration.ofSeconds(30))
-                .build();
+                .timeout(Duration.ofSeconds(30));
+
+            // 添加自定义头
+            if (config.getCustomHeaders() != null) {
+                config.getCustomHeaders().forEach(requestBuilder::header);
+            }
+
+            HttpRequest request = requestBuilder.build();
             
             HttpResponse<String> response = httpClient.send(request, 
                 HttpResponse.BodyHandlers.ofString());
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/OpenAiLLM.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/OpenAiLLM.java
index 855e18b29..7cfdd5799 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/OpenAiLLM.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/llm/impl/OpenAiLLM.java
@@ -148,14 +148,20 @@ public class OpenAiLLM implements LLMBase {
     
     private String sendRequest(Map<String, Object> requestBody) throws Exception {
         String requestJson = gson.toJson(requestBody);
-        
-        HttpRequest request = HttpRequest.newBuilder()
+
+        HttpRequest.Builder requestBuilder = HttpRequest.newBuilder()
             .uri(URI.create(baseUrl + "/chat/completions"))
             .header("Content-Type", "application/json")
             .header("Authorization", "Bearer " + apiKey)
             .POST(HttpRequest.BodyPublishers.ofString(requestJson))
-            .timeout(Duration.ofMinutes(2))
-            .build();
+            .timeout(Duration.ofMinutes(2));
+
+        // 添加自定义头
+        if (config.getCustomHeaders() != null) {
+            config.getCustomHeaders().forEach(requestBuilder::header);
+        }
+
+        HttpRequest request = requestBuilder.build();
             
         HttpResponse<String> response = httpClient.send(request, HttpResponse.BodyHandlers.ofString());
         
@@ -189,7 +195,7 @@ public class OpenAiLLM implements LLMBase {
             JsonObject message = firstChoice.getAsJsonObject("message");
             
             // 检查是否有工具调用
-            if (message.has("tool_calls")) {
+            if (message.has("tool_calls") && message.get("tool_calls").getAsJsonArray().size() > 0) {
                 JsonArray toolCalls = message.getAsJsonArray("tool_calls");
                 // 返回工具调用信息
                 return gson.toJson(Map.of("tool_calls", toolCalls));
@@ -227,11 +233,17 @@ public class OpenAiLLM implements LLMBase {
      */
     public List<String> getAvailableModels() {
         try {
-            HttpRequest request = HttpRequest.newBuilder()
+            HttpRequest.Builder requestBuilder = HttpRequest.newBuilder()
                 .uri(URI.create(baseUrl + "/models"))
                 .header("Authorization", "Bearer " + apiKey)
-                .GET()
-                .build();
+                .GET();
+
+            // 添加自定义头
+            if (config.getCustomHeaders() != null) {
+                config.getCustomHeaders().forEach(requestBuilder::header);
+            }
+
+            HttpRequest request = requestBuilder.build();
                 
             HttpResponse<String> response = httpClient.send(request, HttpResponse.BodyHandlers.ofString());
             
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/utils/MemoryUtils.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/utils/MemoryUtils.java
index d5873d18e..5b7f2776f 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/utils/MemoryUtils.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/utils/MemoryUtils.java
@@ -38,7 +38,7 @@ public class MemoryUtils {
                 JsonArray factsArray = jsonObject.getAsJsonArray("facts");
                 
                 for (int i = 0; i < factsArray.size(); i++) {
-                    String fact = factsArray.get(i).getAsString();
+                    String fact = factsArray.get(i).toString();
                     if (fact != null && !fact.trim().isEmpty()) {
                         facts.add(fact.trim());
                     }
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/vectorstore/VectorStoreFactory.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/vectorstore/VectorStoreFactory.java
index 5ba327ba2..9acbb26a3 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/vectorstore/VectorStoreFactory.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/vectorstore/VectorStoreFactory.java
@@ -23,36 +23,51 @@ public class VectorStoreFactory {
         switch (config.getProvider()) {
             case LOCAL:
                 return new LocalVectorStore(config);
+            case CHROMA:
+                return new ChromaVectorStore(config);
             case QDRANT:
                 return new QdrantVectorStore(config);
-            case CHROMA:
-                throw new UnsupportedOperationException("Chroma not implemented yet");
             case WEAVIATE:
-                throw new UnsupportedOperationException("Weaviate not implemented yet");
+                return new WeaviateVectorStore(config);
             case PINECONE:
-                throw new UnsupportedOperationException("Pinecone not implemented yet");
+                return new PineconeVectorStore(config);
             case FAISS:
-                throw new UnsupportedOperationException("FAISS not implemented yet");
+                return new FaissVectorStore(config);
             case ELASTICSEARCH:
-                throw new UnsupportedOperationException("Elasticsearch not implemented yet");
+                return new ElasticsearchVectorStore(config);
             case REDIS:
-                throw new UnsupportedOperationException("Redis not implemented yet");
+                return new RedisVectorStore(config);
             case PGVECTOR:
-                throw new UnsupportedOperationException("PgVector not implemented yet");
+                return new PgVectorStore(config);
             case MILVUS:
-                throw new UnsupportedOperationException("Milvus not implemented yet");
+                return new MilvusVectorStore(config);
             default:
                 throw new IllegalArgumentException("Unsupported vector store provider: " + config.getProvider());
         }
     }
     
     /**
-     * 创建默认的Qdrant向量存储实例
-     * 
-     * @return Qdrant向量存储实例
+     * 创建默认的向量存储实例（优先本地嵌入式）
+     *
+     * @return 本地向量存储实例
      */
     public static VectorStoreBase createDefault() {
-        return create(VectorStoreConfig.qdrantDefault());
+        return create(VectorStoreConfig.chromaDefault());
+    }
+
+    /**
+     * 为测试创建本地向量存储实例
+     *
+     * @return 本地Chroma向量存储实例
+     */
+    public static VectorStoreBase createLocalForTesting() {
+        return create(VectorStoreConfig.builder()
+                .provider(VectorStoreConfig.Provider.CHROMA)
+                .collectionName("test_collection")
+                .host("localhost")
+                .path("./data/test/chroma")
+                .embeddingModelDims(384)
+                .build());
     }
     
     /**
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/vectorstore/impl/ChromaVectorStore.java b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/vectorstore/impl/ChromaVectorStore.java
index 5d6f2227a..68a190434 100644
--- a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/vectorstore/impl/ChromaVectorStore.java
+++ b/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/vectorstore/impl/ChromaVectorStore.java
@@ -1,61 +1,513 @@
 package run.mone.hive.memory.longterm.vectorstore.impl;
 
 import lombok.extern.slf4j.Slf4j;
+import okhttp3.OkHttpClient;
+import okhttp3.Request;
+import okhttp3.RequestBody;
+import okhttp3.Response;
 import lombok.Data;
 import run.mone.hive.memory.longterm.config.VectorStoreConfig;
 import run.mone.hive.memory.longterm.vectorstore.VectorStoreBase;
 import run.mone.hive.memory.longterm.model.MemoryItem;
-import java.util.List;
-import java.util.Map;
-import java.util.ArrayList;
+
+import tech.amikos.chromadb.Client;
+import tech.amikos.chromadb.Collection;
+import tech.amikos.chromadb.Collection.GetResult;
+import tech.amikos.chromadb.EFException;
+import tech.amikos.chromadb.Embedding;
+import tech.amikos.chromadb.embeddings.DefaultEmbeddingFunction;
+import tech.amikos.chromadb.embeddings.EmbeddingFunction;
+import tech.amikos.chromadb.embeddings.WithParam;
+import tech.amikos.chromadb.embeddings.openai.CreateEmbeddingRequest;
+import tech.amikos.chromadb.embeddings.openai.CreateEmbeddingResponse;
+import tech.amikos.chromadb.handler.ApiException;
+
+import java.util.*;
+import java.util.stream.Collectors;
+
+import com.google.gson.Gson;
+
+import java.io.IOException;
+import java.time.LocalDateTime;
+
+import static tech.amikos.chromadb.Constants.JSON;
 
 @Slf4j
 @Data
+// docker run -d --name chroma-test -p 8000:8000 chromadb/chroma:0.6.4.dev226 for test
 public class ChromaVectorStore implements VectorStoreBase {
     private final VectorStoreConfig config;
-    
+    private Client chromaClient;
+    private Collection collection;
+    private boolean isEmbedded;
+
+    public static final String DEFAULT_EMBEDDING_FUNCTION = "default";
+    public static final String OPENAI_EMBEDDING_FUNCTION = "openai";
+
     public ChromaVectorStore(VectorStoreConfig config) {
         this.config = config;
-        log.info("Chroma vector store initialized with collection: {}", config.getCollectionName());
+        this.isEmbedded = initializeClient();
+        this.collection = createOrGetCollection();
+
+        log.info("Chroma vector store initialized {} with collection: {}",
+                isEmbedded ? "in embedded mode" : "with remote connection",
+                config.getCollectionName());
     }
-    
+
+    private boolean initializeClient() {
+        try {
+            if (config.getHost() == null || "localhost".equals(config.getHost()) ||
+                "127.0.0.1".equals(config.getHost()) || config.getHost().isEmpty()) {
+
+                // For local embedded mode, use default URL
+                chromaClient = new Client("http://localhost:8000");
+                log.info("Initialized embedded Chroma client with default URL");
+                return true;
+            } else {
+                String url = "http://" + config.getHost() + ":" + config.getPort();
+                chromaClient = new Client(url);
+                log.info("Initialized Chroma client for URL: {}", url);
+                return false;
+            }
+        } catch (Exception e) {
+            log.error("Failed to initialize Chroma client: {}", e.getMessage());
+            throw new RuntimeException("Failed to initialize Chroma client", e);
+        }
+    }
+
+    private Collection createOrGetCollection() {
+        try {
+            String collectionName = config.getCollectionName();
+            EmbeddingFunction ef = config.getEmbeddingFunction() != null && config.getEmbeddingFunction().equals(OPENAI_EMBEDDING_FUNCTION)
+            ? new OpenAIEmbeddingFunction(WithParam.apiKey(config.getApiKey()), WithParam.model("text-embedding-3-small"), WithParam.baseAPI(config.getBaseUrl()))
+            : new DefaultEmbeddingFunction();
+
+            try {
+                return chromaClient.getCollection(collectionName, ef);
+            } catch (ApiException e) {
+                if (e.getCode() == 404 || e.getCode() == 400) {
+                    log.info("Collection {} not found, creating new one", collectionName);
+                    Map<String, String> metadata = new HashMap<>();
+                    metadata.put("hnsw:space", "cosine");
+                    return chromaClient.createCollection(collectionName, metadata, true, ef);
+                } else {
+                    throw e;
+                }
+            }
+        } catch (Exception e) {
+            log.error("Failed to create/get collection {}: {}", config.getCollectionName(), e.getMessage());
+            throw new RuntimeException("Failed to create/get collection", e);
+        }
+    }
+
     @Override
     public void insert(List<List<Double>> vectors, List<String> ids, List<Map<String, Object>> payloads) {
-        throw new UnsupportedOperationException("Chroma vector store implementation coming soon");
+        try {
+            List<Map<String, String>> metadatas = new ArrayList<>();
+            List<String> documents = new ArrayList<>();
+
+            for (int i = 0; i < payloads.size(); i++) {
+                Map<String, Object> payload = payloads.get(i);
+                Map<String, String> metadata = new HashMap<>();
+                for (Map.Entry<String, Object> entry : payload.entrySet()) {
+                    metadata.put(entry.getKey(), entry.getValue() != null ? entry.getValue().toString() : "");
+                }
+                metadata.put("timestamp", String.valueOf(System.currentTimeMillis()));
+                metadata.put("collection", config.getCollectionName());
+                metadatas.add(metadata);
+
+                String document = payload.get("memory") != null ? payload.get("memory").toString() : "";
+                documents.add(document);
+            }
+
+            // Convert vectors to the format expected by ChromaDB
+            List<Embedding> embeddings = new ArrayList<>();
+            for (List<Double> vector : vectors) {
+                embeddings.add(new Embedding(vector));
+            }
+
+            collection.add(
+                embeddings,
+                metadatas,
+                documents,
+                ids
+            );
+
+            log.info("Inserting {} vectors into collection {}", vectors.size(), config.getCollectionName());
+        } catch (Exception e) {
+            log.error("Failed to insert vectors: {}", e.getMessage());
+            throw new RuntimeException("Failed to insert vectors", e);
+        }
     }
-    
+
     @Override
     public List<MemoryItem> search(String query, List<Double> vectors, int limit, Map<String, Object> filters) {
-        throw new UnsupportedOperationException("Chroma vector store implementation coming soon");
+        try {
+            Map<String, Object> whereClause = generateWhereClause(filters);
+
+            Collection.QueryResponse response = collection.query(
+                List.of(query),
+                limit,
+                whereClause,
+                null,
+                null
+            );
+
+            List<MemoryItem> results = new ArrayList<>();
+            List<String> ids = response.getIds().get(0);
+            List<List<Float>> distances = response.getDistances();
+            List<Map<String, Object>> metadatas = response.getMetadatas().get(0);
+            List<String> documents = response.getDocuments().get(0);
+
+            for (int i = 0; i < ids.size(); i++) {
+                String id = ids.get(i);
+                double distance = distances.get(0).get(i).doubleValue();
+                double similarity = 1.0 - distance;  // Convert distance to similarity
+                Map<String, Object> metadata = metadatas.get(i);
+                String document = documents.get(i);
+
+                MemoryItem item = MemoryItem.builder()
+                        .id(id)
+                        .memory(document)
+                        .score(similarity)
+                        .userId(getStringFromMetadata(metadata, "user_id"))
+                        .agentId(getStringFromMetadata(metadata, "agent_id"))
+                        .runId(getStringFromMetadata(metadata, "run_id"))
+                        .actorId(getStringFromMetadata(metadata, "actor_id"))
+                        .role(getStringFromMetadata(metadata, "role"))
+                        .metadata(metadata)
+                        .createdAt(LocalDateTime.now())
+                        .updatedAt(LocalDateTime.now())
+                        .build();
+
+                results.add(item);
+            }
+
+            log.debug("Found {} similar vectors in Chroma collection {}", results.size(), config.getCollectionName());
+            return results;
+        } catch (Exception e) {
+            log.error("Failed to search vectors: {}", e.getMessage());
+            return new ArrayList<>();
+        }
     }
-    
+
+    private String getStringFromMetadata(Map<String, Object> metadata, String key) {
+        Object value = metadata.get(key);
+        return value != null ? value.toString() : null;
+    }
+
+    /**
+     * Generate a properly formatted where clause for ChromaDB.
+     *
+     * @param filters The filter conditions.
+     * @return Properly formatted where clause for ChromaDB.
+     */
+    private Map<String, Object> generateWhereClause(Map<String, Object> filters) {
+        if (filters == null || filters.isEmpty()) {
+            return new HashMap<>();
+        }
+
+        // If only one filter is supplied, return it as is
+        // (no need to wrap in $and based on chroma docs)
+        if (filters.size() <= 1) {
+            return new HashMap<>(filters);
+        }
+
+        List<Map<String, Object>> whereFilters = new ArrayList<>();
+        for (Map.Entry<String, Object> entry : filters.entrySet()) {
+            if (entry.getValue() instanceof String) {
+                Map<String, Object> filter = new HashMap<>();
+                filter.put(entry.getKey(), entry.getValue());
+                whereFilters.add(filter);
+            }
+        }
+
+        Map<String, Object> result = new HashMap<>();
+        result.put("$and", whereFilters);
+        return result;
+    }
+
+    /**
+     * Parse a single item from GetResponse.
+     *
+     * @param response The GetResponse from ChromaDB.
+     * @param index The index of the item to parse.
+     * @return Parsed MemoryItem object.
+     */
+    private MemoryItem parseGetResponse(GetResult response, int index) {
+        List<String> ids = response.getIds();
+        List<Map<String, Object>> metadatas = response.getMetadatas();
+        List<String> documents = response.getDocuments();
+
+        if (index >= ids.size()) {
+            return null;
+        }
+
+        String id = ids.get(index);
+        Map<String, Object> metadata = index < metadatas.size() ? metadatas.get(index) : new HashMap<>();
+        String document = index < documents.size() ? documents.get(index) : "";
+
+        return MemoryItem.builder()
+                .id(id)
+                .memory(document)
+                .score(1.0) // No similarity score available in get operation
+                .userId(getStringFromMetadata(metadata, "user_id"))
+                .agentId(getStringFromMetadata(metadata, "agent_id"))
+                .runId(getStringFromMetadata(metadata, "run_id"))
+                .actorId(getStringFromMetadata(metadata, "actor_id"))
+                .role(getStringFromMetadata(metadata, "role"))
+                .metadata(metadata)
+                .createdAt(LocalDateTime.now())
+                .updatedAt(LocalDateTime.now())
+                .build();
+    }
+
+    /**
+     * Parse all items from GetResponse.
+     *
+     * @param response The GetResponse from ChromaDB.
+     * @return List of parsed MemoryItem objects.
+     */
+    private List<MemoryItem> parseGetResponseList(GetResult response) {
+        List<MemoryItem> results = new ArrayList<>();
+
+        if (response == null || response.getIds() == null || response.getIds().isEmpty()) {
+            return results;
+        }
+
+        List<String> ids = response.getIds();
+        List<Map<String, Object>> metadatas = response.getMetadatas() != null ? response.getMetadatas() : new ArrayList<>();
+        List<String> documents = response.getDocuments() != null ? response.getDocuments() : new ArrayList<>();
+
+        for (int i = 0; i < ids.size(); i++) {
+            String id = ids.get(i);
+            Map<String, Object> metadata = i < metadatas.size() ? metadatas.get(i) : new HashMap<>();
+            String document = i < documents.size() ? documents.get(i) : "";
+
+            MemoryItem item = MemoryItem.builder()
+                    .id(id)
+                    .memory(document)
+                    .score(1.0) // No similarity score available in list operation
+                    .userId(getStringFromMetadata(metadata, "user_id"))
+                    .agentId(getStringFromMetadata(metadata, "agent_id"))
+                    .runId(getStringFromMetadata(metadata, "run_id"))
+                    .actorId(getStringFromMetadata(metadata, "actor_id"))
+                    .role(getStringFromMetadata(metadata, "role"))
+                    .metadata(metadata)
+                    .createdAt(LocalDateTime.now())
+                    .updatedAt(LocalDateTime.now())
+                    .build();
+
+            results.add(item);
+        }
+
+        log.debug("Listed {} vectors from Chroma collection {}", results.size(), config.getCollectionName());
+        return results;
+    }
+
     @Override
     public MemoryItem get(String vectorId) {
-        throw new UnsupportedOperationException("Chroma vector store implementation coming soon");
+        try {
+            GetResult result = collection.get((List.of(vectorId)), null, null);
+            if (result != null && result.getIds() != null && !result.getIds().isEmpty()) {
+                return parseGetResponse(result, 0);
+            }
+            return null;
+        } catch (Exception e) {
+            log.error("Failed to get vector {}: {}", vectorId, e.getMessage());
+            return null;
+        }
     }
-    
+
     @Override
     public List<MemoryItem> list(Map<String, Object> filters, int limit) {
-        return new ArrayList<>();
+        try {
+            Map<String, Object> whereClause = generateWhereClause(filters);
+            GetResult result = collection.get(null, null, whereClause);
+            return parseGetResponseList(result);
+        } catch (Exception e) {
+            log.error("Failed to list vectors: {}", e.getMessage());
+            return new ArrayList<>();
+        }
     }
-    
+
     @Override
     public void update(String vectorId, List<Double> vector, Map<String, Object> payload) {
-        throw new UnsupportedOperationException("Chroma vector store implementation coming soon");
+        try {
+            Map<String, Object> metadata = new HashMap<>(payload);
+            metadata.put("timestamp", System.currentTimeMillis());
+            metadata.put("collection", config.getCollectionName());
+
+            collection.update(
+                vectorId,
+                metadata
+            );
+
+            log.debug("Updated vector {} in Chroma collection {}", vectorId, config.getCollectionName());
+        } catch (Exception e) {
+            log.error("Failed to update vector {}: {}", vectorId, e.getMessage());
+            throw new RuntimeException("Failed to update vector", e);
+        }
     }
-    
+
     @Override
     public void delete(String vectorId) {
-        throw new UnsupportedOperationException("Chroma vector store implementation coming soon");
+        try {
+            collection.delete(Arrays.asList(vectorId), null, null);
+            log.debug("Deleted vector {} from Chroma collection {}", vectorId, config.getCollectionName());
+        } catch (Exception e) {
+            log.error("Failed to delete vector {}: {}", vectorId, e.getMessage());
+            throw new RuntimeException("Failed to delete vector", e);
+        }
     }
-    
+
     @Override
     public void deleteCol() {
-        throw new UnsupportedOperationException("Chroma vector store implementation coming soon");
+        try {
+            chromaClient.deleteCollection(config.getCollectionName());
+            log.info("Deleted Chroma collection {}", config.getCollectionName());
+        } catch (Exception e) {
+            log.error("Failed to delete collection {}: {}", config.getCollectionName(), e.getMessage());
+            throw new RuntimeException("Failed to delete collection", e);
+        }
     }
-    
+
     @Override
     public void reset() {
-        throw new UnsupportedOperationException("Chroma vector store implementation coming soon");
+        try {
+            deleteCol();
+            collection = createOrGetCollection();
+            log.info("Reset Chroma collection {}", config.getCollectionName());
+        } catch (Exception e) {
+            log.error("Failed to reset collection {}: {}", config.getCollectionName(), e.getMessage());
+            throw new RuntimeException("Failed to reset collection", e);
+        }
+    }
+
+    @Override
+    public boolean collectionExists() {
+        try {
+            chromaClient.getCollection(config.getCollectionName(), null);
+            return true;
+        } catch (Exception e) {
+            return false;
+        }
+    }
+
+    @Override
+    public void createCollection() {
+        if (!collectionExists()) {
+            collection = createOrGetCollection();
+        }
+    }
+
+    @Override
+    public long getVectorCount() {
+        try {
+            return collection.count();
+        } catch (Exception e) {
+            log.error("Failed to get vector count: {}", e.getMessage());
+            return 0;
+        }
+    }
+
+    @Override
+    public Map<String, Object> getStats() {
+        Map<String, Object> stats = new HashMap<>();
+        stats.put("vector_count", getVectorCount());
+        stats.put("collection_name", config.getCollectionName());
+        stats.put("provider", "chroma");
+        stats.put("embedded_mode", isEmbedded);
+        stats.put("dimensions", config.getEmbeddingModelDims());
+        return stats;
+    }
+
+    @Override
+    public void close() {
+        try {
+            if (chromaClient != null) {
+                // ChromaDB Java client doesn't have a close method
+                // Just null the reference to help with garbage collection
+                chromaClient = null;
+            }
+            log.info("Closed Chroma vector store for collection {}", config.getCollectionName());
+        } catch (Exception e) {
+            log.warn("Error closing Chroma client: {}", e.getMessage());
+        }
+    }
+
+    public static class OpenAIEmbeddingFunction implements EmbeddingFunction {
+        public static final String DEFAULT_MODEL_NAME = "text-embedding-ada-002";
+        public static final String DEFAULT_BASE_API = "https://api.openai.com/v1/embeddings";
+        public static final String OPENAI_API_KEY_ENV = "OPENAI_API_KEY";
+        private final OkHttpClient client = new OkHttpClient();
+        private final Gson gson = new Gson();
+        private final Map<String, Object> configParams = new HashMap<>();
+        private static final List<WithParam> defaults = Arrays.asList(
+                WithParam.baseAPI(DEFAULT_BASE_API),
+                WithParam.defaultModel(DEFAULT_MODEL_NAME)
+        );
+    
+    
+        public OpenAIEmbeddingFunction() throws Exception {
+            for (WithParam param : defaults) {
+                param.apply(this.configParams);
+            }
+            WithParam.apiKeyFromEnv(OPENAI_API_KEY_ENV).apply(this.configParams);
+        }
+    
+        public OpenAIEmbeddingFunction(WithParam... params) throws Exception {
+            // apply defaults
+    
+            for (WithParam param : defaults) {
+                param.apply(this.configParams);
+            }
+            for (WithParam param : params) {
+                param.apply(this.configParams);
+            }
+        }
+    
+        public CreateEmbeddingResponse createEmbedding(CreateEmbeddingRequest req) throws EFException {
+            Request request = new Request.Builder()
+                    .url(this.configParams.get("baseAPI").toString())
+                    .post(RequestBody.create(req.json(), JSON))
+                    .addHeader("Accept", "application/json")
+                    .addHeader("Content-Type", "application/json")
+                    .addHeader("Authorization", "Bearer " + configParams.get("apiKey").toString())
+                    .addHeader("X-Model-Provider-Id", "azure_openai")
+                    .build();
+            try (Response response = client.newCall(request).execute()) {
+                if (!response.isSuccessful()) {
+                    throw new IOException("Unexpected code " + response);
+                }
+    
+                String responseData = response.body().string();
+    
+                return gson.fromJson(responseData, CreateEmbeddingResponse.class);
+            } catch (IOException e) {
+                throw new EFException(e);
+            }
+        }
+    
+        @Override
+        public Embedding embedQuery(String query) throws EFException {
+            CreateEmbeddingRequest req = new CreateEmbeddingRequest().model(this.configParams.get("modelName").toString());
+            req.input(new CreateEmbeddingRequest.Input(query));
+            CreateEmbeddingResponse response = this.createEmbedding(req);
+            return new Embedding(response.getData().get(0).getEmbedding());
+        }
+    
+        @Override
+        public List<Embedding> embedDocuments(List<String> documents) throws EFException {
+            CreateEmbeddingRequest req = new CreateEmbeddingRequest().model(this.configParams.get("modelName").toString());
+            req.input(new CreateEmbeddingRequest.Input(documents.toArray(new String[0])));
+            CreateEmbeddingResponse response = this.createEmbedding(req);
+            return response.getData().stream().map(emb -> new Embedding(emb.getEmbedding())).collect(Collectors.toList());
+        }
+    
+        @Override
+        public List<Embedding> embedDocuments(String[] documents) throws EFException {
+            return embedDocuments(Arrays.asList(documents));
+        }
     }
-}
+}
\ No newline at end of file
diff --git a/jcommon/hive/src/main/resources/test_chroma.sh b/jcommon/hive/src/main/resources/test_chroma.sh
new file mode 100755
index 000000000..39bc66968
--- /dev/null
+++ b/jcommon/hive/src/main/resources/test_chroma.sh
@@ -0,0 +1,13 @@
+#!/bin/bash
+
+echo "Testing ChromaDB connection..."
+response=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8000/api/v1/heartbeat)
+
+if [ "$response" = "200" ]; then
+    echo "✅ ChromaDB is running and responsive"
+    echo "Heartbeat response:"
+    curl -s http://localhost:8000/api/v1/heartbeat | python -m json.tool
+else
+    echo "❌ ChromaDB is not responding (HTTP code: $response)"
+    exit 1
+fi
diff --git a/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/ChromaLocalTest.java b/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/ChromaLocalTest.java
new file mode 100644
index 000000000..781866d41
--- /dev/null
+++ b/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/ChromaLocalTest.java
@@ -0,0 +1,303 @@
+package run.mone.hive.memory.longterm;
+
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.AfterEach;
+import org.junit.jupiter.api.DisplayName;
+import static org.junit.jupiter.api.Assertions.*;
+
+import run.mone.hive.memory.longterm.config.VectorStoreConfig;
+import run.mone.hive.memory.longterm.vectorstore.VectorStoreFactory;
+import run.mone.hive.memory.longterm.vectorstore.VectorStoreBase;
+import run.mone.hive.memory.longterm.model.MemoryItem;
+
+import java.util.*;
+import java.io.File;
+
+@DisplayName("Chroma Local Embedded Vector Store Tests")
+public class ChromaLocalTest {
+
+    private VectorStoreBase vectorStore;
+    private VectorStoreConfig config;
+    private String testPath = "./data/test/chroma_test";
+
+    @BeforeEach
+    void setUp() {
+        // 清理测试目录
+        cleanupTestDirectory();
+
+        config = VectorStoreConfig.builder()
+                .provider(VectorStoreConfig.Provider.CHROMA)
+                .collectionName("test_collection")
+                .host("localhost")
+                .path(testPath)
+                .embeddingModelDims(384)
+                .build();
+
+        vectorStore = VectorStoreFactory.create(config);
+    }
+
+    @AfterEach
+    void tearDown() {
+        if (vectorStore != null) {
+            vectorStore.close();
+        }
+        cleanupTestDirectory();
+    }
+
+    private void cleanupTestDirectory() {
+        File testDir = new File(testPath);
+        if (testDir.exists()) {
+            deleteDirectory(testDir);
+        }
+    }
+
+    private void deleteDirectory(File directory) {
+        File[] files = directory.listFiles();
+        if (files != null) {
+            for (File file : files) {
+                if (file.isDirectory()) {
+                    deleteDirectory(file);
+                } else {
+                    file.delete();
+                }
+            }
+        }
+        directory.delete();
+    }
+
+    @Test
+    @DisplayName("Should initialize Chroma embedded store successfully")
+    void testInitialization() {
+        assertNotNull(vectorStore);
+        assertTrue(vectorStore.validateConnection());
+
+        Map<String, Object> stats = vectorStore.getStats();
+        assertEquals("chroma", stats.get("provider"));
+        assertTrue((Boolean) stats.get("embedded_mode"));
+        assertEquals(0L, stats.get("vector_count"));
+    }
+
+    @Test
+    @DisplayName("Should create collection if it doesn't exist")
+    void testCollectionCreation() {
+        vectorStore.createCollection();
+        assertTrue(vectorStore.collectionExists());
+    }
+
+    @Test
+    @DisplayName("Should insert and retrieve vectors successfully")
+    void testInsertAndRetrieve() {
+        // 准备测试数据
+        List<List<Double>> vectors = Arrays.asList(
+            Arrays.asList(0.1, 0.2, 0.3, 0.4),
+            Arrays.asList(0.5, 0.6, 0.7, 0.8)
+        );
+
+        List<String> ids = Arrays.asList("test1", "test2");
+
+        List<Map<String, Object>> payloads = Arrays.asList(
+            createPayload("First test memory", "user1", "agent1"),
+            createPayload("Second test memory", "user1", "agent1")
+        );
+
+        // 插入向量
+        vectorStore.insert(vectors, ids, payloads);
+
+        // 验证插入
+        assertEquals(2L, vectorStore.getVectorCount());
+
+        // 检索特定向量
+        MemoryItem item1 = vectorStore.get("test1");
+        assertNotNull(item1);
+        assertEquals("test1", item1.getId());
+        assertEquals("First test memory", item1.getMemory());
+        assertEquals("user1", item1.getUserId());
+        assertEquals("agent1", item1.getAgentId());
+
+        MemoryItem item2 = vectorStore.get("test2");
+        assertNotNull(item2);
+        assertEquals("test2", item2.getId());
+        assertEquals("Second test memory", item2.getMemory());
+    }
+
+    @Test
+    @DisplayName("Should perform vector similarity search")
+    void testVectorSearch() {
+        // 插入测试数据
+        insertTestVectors();
+
+        // 执行相似性搜索
+        List<Double> queryVector = Arrays.asList(0.15, 0.25, 0.35, 0.45);
+        List<MemoryItem> results = vectorStore.search("test", queryVector, 5, null);
+
+        // 验证搜索结果
+        assertFalse(results.isEmpty());
+        assertTrue(results.size() <= 5);
+
+        // 验证结果按相似度排序
+        for (int i = 0; i < results.size() - 1; i++) {
+            assertTrue(results.get(i).getScore() >= results.get(i + 1).getScore());
+        }
+    }
+
+    @Test
+    @DisplayName("Should support filtering in search")
+    void testSearchWithFilters() {
+        // 插入不同用户的测试数据
+        insertTestVectorsWithDifferentUsers();
+
+        // 使用过滤器搜索
+        Map<String, Object> filters = Map.of("user_id", "user1");
+        List<Double> queryVector = Arrays.asList(0.1, 0.2, 0.3, 0.4);
+        List<MemoryItem> results = vectorStore.search("test", queryVector, 10, filters);
+
+        // 验证所有结果都属于指定用户
+        for (MemoryItem item : results) {
+            assertEquals("user1", item.getUserId());
+        }
+    }
+
+    @Test
+    @DisplayName("Should list vectors with filters")
+    void testListWithFilters() {
+        insertTestVectorsWithDifferentUsers();
+
+        // 列出特定用户的向量
+        Map<String, Object> filters = Map.of("user_id", "user2");
+        List<MemoryItem> results = vectorStore.list(filters, 10);
+
+        assertFalse(results.isEmpty());
+        for (MemoryItem item : results) {
+            assertEquals("user2", item.getUserId());
+        }
+    }
+
+    @Test
+    @DisplayName("Should update vectors successfully")
+    void testUpdateVector() {
+        insertTestVectors();
+
+        // 更新向量
+        List<Double> newVector = Arrays.asList(0.9, 0.8, 0.7, 0.6);
+        Map<String, Object> newPayload = createPayload("Updated memory", "user1", "agent1");
+
+        vectorStore.update("test1", newVector, newPayload);
+
+        // 验证更新
+        MemoryItem updatedItem = vectorStore.get("test1");
+        assertNotNull(updatedItem);
+        assertEquals("Updated memory", updatedItem.getMemory());
+    }
+
+    @Test
+    @DisplayName("Should delete vectors successfully")
+    void testDeleteVector() {
+        insertTestVectors();
+
+        // 删除向量
+        vectorStore.delete("test1");
+
+        // 验证删除
+        MemoryItem deletedItem = vectorStore.get("test1");
+        assertNull(deletedItem);
+        assertEquals(1L, vectorStore.getVectorCount());
+    }
+
+    @Test
+    @DisplayName("Should reset collection successfully")
+    void testResetCollection() {
+        insertTestVectors();
+        assertEquals(2L, vectorStore.getVectorCount());
+
+        // 重置集合
+        vectorStore.reset();
+
+        // 验证重置
+        assertEquals(0L, vectorStore.getVectorCount());
+        assertTrue(vectorStore.collectionExists());
+    }
+
+    @Test
+    @DisplayName("Should delete collection successfully")
+    void testDeleteCollection() {
+        insertTestVectors();
+        assertTrue(vectorStore.collectionExists());
+
+        // 删除集合
+        vectorStore.deleteCol();
+
+        // 验证删除
+        assertFalse(vectorStore.collectionExists());
+    }
+
+    @Test
+    @DisplayName("Should handle empty searches gracefully")
+    void testEmptySearch() {
+        // 在空集合中搜索
+        List<Double> queryVector = Arrays.asList(0.1, 0.2, 0.3, 0.4);
+        List<MemoryItem> results = vectorStore.search("test", queryVector, 5, null);
+
+        assertTrue(results.isEmpty());
+    }
+
+    @Test
+    @DisplayName("Should provide accurate statistics")
+    void testGetStats() {
+        insertTestVectors();
+
+        Map<String, Object> stats = vectorStore.getStats();
+
+        assertEquals(2L, stats.get("vector_count"));
+        assertEquals("test_collection", stats.get("collection_name"));
+        assertEquals("chroma", stats.get("provider"));
+        assertTrue((Boolean) stats.get("embedded_mode"));
+        assertEquals(384, stats.get("dimensions"));
+    }
+
+    private void insertTestVectors() {
+        List<List<Double>> vectors = Arrays.asList(
+            Arrays.asList(0.1, 0.2, 0.3, 0.4),
+            Arrays.asList(0.5, 0.6, 0.7, 0.8)
+        );
+
+        List<String> ids = Arrays.asList("test1", "test2");
+
+        List<Map<String, Object>> payloads = Arrays.asList(
+            createPayload("First test memory", "user1", "agent1"),
+            createPayload("Second test memory", "user1", "agent1")
+        );
+
+        vectorStore.insert(vectors, ids, payloads);
+    }
+
+    private void insertTestVectorsWithDifferentUsers() {
+        List<List<Double>> vectors = Arrays.asList(
+            Arrays.asList(0.1, 0.2, 0.3, 0.4),
+            Arrays.asList(0.5, 0.6, 0.7, 0.8),
+            Arrays.asList(0.2, 0.3, 0.4, 0.5),
+            Arrays.asList(0.6, 0.7, 0.8, 0.9)
+        );
+
+        List<String> ids = Arrays.asList("test1", "test2", "test3", "test4");
+
+        List<Map<String, Object>> payloads = Arrays.asList(
+            createPayload("User1 memory 1", "user1", "agent1"),
+            createPayload("User1 memory 2", "user1", "agent1"),
+            createPayload("User2 memory 1", "user2", "agent1"),
+            createPayload("User2 memory 2", "user2", "agent1")
+        );
+
+        vectorStore.insert(vectors, ids, payloads);
+    }
+
+    private Map<String, Object> createPayload(String memory, String userId, String agentId) {
+        Map<String, Object> payload = new HashMap<>();
+        payload.put("memory", memory);
+        payload.put("user_id", userId);
+        payload.put("agent_id", agentId);
+        payload.put("run_id", "test_run");
+        payload.put("role", "user");
+        return payload;
+    }
+}
\ No newline at end of file
diff --git a/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/KuzuLocalTest.java b/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/KuzuLocalTest.java
new file mode 100644
index 000000000..fa7521264
--- /dev/null
+++ b/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/KuzuLocalTest.java
@@ -0,0 +1,322 @@
+package run.mone.hive.memory.longterm;
+
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.AfterEach;
+import org.junit.jupiter.api.DisplayName;
+import static org.junit.jupiter.api.Assertions.*;
+
+import run.mone.hive.memory.longterm.config.GraphStoreConfig;
+import run.mone.hive.memory.longterm.graph.GraphStoreFactory;
+import run.mone.hive.memory.longterm.graph.GraphStoreBase;
+import run.mone.hive.memory.longterm.graph.GraphStoreBase.GraphEntity;
+
+import java.util.*;
+import java.io.File;
+
+@DisplayName("Kuzu Local Embedded Graph Store Tests")
+public class KuzuLocalTest {
+
+    private GraphStoreBase graphStore;
+    private GraphStoreConfig config;
+    private String testPath = "./data/test/kuzu_test";
+
+    @BeforeEach
+    void setUp() {
+        // 清理测试目录
+        cleanupTestDirectory();
+
+        config = GraphStoreConfig.builder()
+                .provider(GraphStoreConfig.Provider.KUZU)
+                .url(testPath)
+                .enabled(true)
+                .build();
+
+        graphStore = GraphStoreFactory.create(config);
+    }
+
+    @AfterEach
+    void tearDown() {
+        if (graphStore != null) {
+            graphStore.close();
+        }
+        cleanupTestDirectory();
+    }
+
+    private void cleanupTestDirectory() {
+        File testDir = new File(testPath);
+        if (testDir.exists()) {
+            deleteDirectory(testDir);
+        }
+    }
+
+    private void deleteDirectory(File directory) {
+        File[] files = directory.listFiles();
+        if (files != null) {
+            for (File file : files) {
+                if (file.isDirectory()) {
+                    deleteDirectory(file);
+                } else {
+                    file.delete();
+                }
+            }
+        }
+        directory.delete();
+    }
+
+    @Test
+    @DisplayName("Should initialize Kuzu embedded store successfully")
+    void testInitialization() {
+        assertNotNull(graphStore);
+        assertTrue(graphStore.validateConnection());
+
+        Map<String, Object> stats = graphStore.getStats();
+        assertEquals("kuzu", stats.get("provider"));
+        assertTrue((Boolean) stats.get("embedded_mode"));
+        assertEquals(0L, stats.get("node_count"));
+        assertEquals(0L, stats.get("relationship_count"));
+    }
+
+    @Test
+    @DisplayName("Should add graph memory successfully")
+    void testAddMemory() {
+        Map<String, Object> result = graphStore.addMemory(
+            "Alice", "Bob", "KNOWS", "PERSON", "PERSON"
+        );
+
+        assertEquals("add_graph_memory", result.get("action"));
+        assertEquals("success", result.get("status"));
+        assertEquals("Alice", result.get("source"));
+        assertEquals("Bob", result.get("destination"));
+        assertEquals("KNOWS", result.get("relationship"));
+
+        // 验证关系存在
+        assertTrue(graphStore.relationshipExists("Alice", "Bob", "KNOWS"));
+    }
+
+    @Test
+    @DisplayName("Should update graph memory successfully")
+    void testUpdateMemory() {
+        // 先添加关系
+        graphStore.addMemory("Alice", "Bob", "KNOWS", "PERSON", "PERSON");
+
+        // 更新关系
+        Map<String, Object> result = graphStore.updateMemory("Alice", "Bob", "FRIEND_OF");
+
+        assertEquals("update_graph_memory", result.get("action"));
+        assertEquals("success", result.get("status"));
+        assertEquals("FRIEND_OF", result.get("relationship"));
+
+        // 验证关系已更新（原关系不存在，新关系存在）
+        assertFalse(graphStore.relationshipExists("Alice", "Bob", "KNOWS"));
+        assertTrue(graphStore.relationshipExists("Alice", "Bob", "FRIEND_OF"));
+    }
+
+    @Test
+    @DisplayName("Should delete graph memory successfully")
+    void testDeleteMemory() {
+        // 先添加关系
+        graphStore.addMemory("Alice", "Bob", "KNOWS", "PERSON", "PERSON");
+        assertTrue(graphStore.relationshipExists("Alice", "Bob", "KNOWS"));
+
+        // 删除关系
+        Map<String, Object> result = graphStore.deleteMemory("Alice", "Bob", "KNOWS");
+
+        assertEquals("delete_graph_memory", result.get("action"));
+        assertEquals("success", result.get("status"));
+
+        // 验证关系已删除
+        assertFalse(graphStore.relationshipExists("Alice", "Bob", "KNOWS"));
+    }
+
+    @Test
+    @DisplayName("Should search relationships successfully")
+    void testSearchRelationships() {
+        // 添加测试数据
+        addTestRelationships();
+
+        // 搜索包含 "Alice" 的关系
+        List<Map<String, Object>> results = graphStore.search("Alice", 10);
+
+        assertFalse(results.isEmpty());
+
+        // 验证搜索结果包含预期的关系
+        boolean foundAliceBob = results.stream().anyMatch(r ->
+            "Alice".equals(r.get("source")) && "Bob".equals(r.get("destination"))
+        );
+        assertTrue(foundAliceBob);
+    }
+
+    @Test
+    @DisplayName("Should get all relationships with limit")
+    void testGetAllRelationships() {
+        addTestRelationships();
+
+        List<Map<String, Object>> results = graphStore.getAll(5);
+
+        assertFalse(results.isEmpty());
+        assertTrue(results.size() <= 5);
+
+        // 验证结果包含必要字段
+        for (Map<String, Object> result : results) {
+            assertNotNull(result.get("source"));
+            assertNotNull(result.get("destination"));
+            assertNotNull(result.get("relationship"));
+        }
+    }
+
+    @Test
+    @DisplayName("Should extract entities from text")
+    void testExtractEntities() {
+        String text = "Alice works at Google and knows Bob from Microsoft";
+        List<Map<String, Object>> entities = graphStore.extractEntities(text);
+
+        assertFalse(entities.isEmpty());
+
+        // 验证提取的实体
+        Set<String> entityNames = new HashSet<>();
+        for (Map<String, Object> entity : entities) {
+            entityNames.add((String) entity.get("name"));
+            assertEquals("GENERAL", entity.get("type"));
+        }
+
+        assertTrue(entityNames.contains("Alice"));
+        assertTrue(entityNames.contains("Google"));
+        assertTrue(entityNames.contains("Bob"));
+        assertTrue(entityNames.contains("Microsoft"));
+    }
+
+    @Test
+    @DisplayName("Should establish relations from text")
+    void testEstablishRelations() {
+        String text = "Alice is a developer and Bob is a manager";
+        List<GraphEntity> relations = graphStore.establishRelations(text);
+
+        assertFalse(relations.isEmpty());
+
+        // 验证建立的关系
+        GraphEntity relation = relations.get(0);
+        assertEquals("Alice", relation.getSource());
+        assertEquals("IS", relation.getRelationship());
+        assertEquals("GENERAL", relation.getSourceType());
+        assertEquals("GENERAL", relation.getDestinationType());
+    }
+
+    @Test
+    @DisplayName("Should get node relationships")
+    void testGetNodeRelationships() {
+        // 添加多个关系
+        graphStore.addMemory("Alice", "Bob", "KNOWS", "PERSON", "PERSON");
+        graphStore.addMemory("Alice", "Charlie", "WORKS_WITH", "PERSON", "PERSON");
+        graphStore.addMemory("David", "Alice", "MANAGES", "PERSON", "PERSON");
+
+        List<Map<String, Object>> relationships = graphStore.getNodeRelationships("Alice");
+
+        assertFalse(relationships.isEmpty());
+        assertEquals(3, relationships.size());
+
+        // 验证关系方向
+        Map<String, Integer> directionCount = new HashMap<>();
+        for (Map<String, Object> rel : relationships) {
+            String direction = (String) rel.get("direction");
+            directionCount.merge(direction, 1, Integer::sum);
+        }
+
+        assertTrue(directionCount.containsKey("outgoing"));
+        assertTrue(directionCount.containsKey("incoming"));
+    }
+
+    @Test
+    @DisplayName("Should handle batch memory addition")
+    void testBatchAddMemories() {
+        List<GraphEntity> entities = Arrays.asList(
+            new GraphEntity("Alice", "Bob", "KNOWS", "PERSON", "PERSON"),
+            new GraphEntity("Bob", "Charlie", "WORKS_WITH", "PERSON", "PERSON"),
+            new GraphEntity("Charlie", "Alice", "REPORTS_TO", "PERSON", "PERSON")
+        );
+
+        List<Map<String, Object>> results = graphStore.addMemories(entities);
+
+        assertEquals(3, results.size());
+        for (Map<String, Object> result : results) {
+            assertEquals("success", result.get("status"));
+        }
+
+        // 验证所有关系都已添加
+        assertTrue(graphStore.relationshipExists("Alice", "Bob", "KNOWS"));
+        assertTrue(graphStore.relationshipExists("Bob", "Charlie", "WORKS_WITH"));
+        assertTrue(graphStore.relationshipExists("Charlie", "Alice", "REPORTS_TO"));
+    }
+
+    @Test
+    @DisplayName("Should delete all data successfully")
+    void testDeleteAll() {
+        addTestRelationships();
+
+        // 验证数据存在
+        Map<String, Object> statsBefore = graphStore.getStats();
+        assertTrue((Long) statsBefore.get("node_count") > 0);
+        assertTrue((Long) statsBefore.get("relationship_count") > 0);
+
+        // 删除所有数据
+        graphStore.deleteAll();
+
+        // 验证数据已清空
+        Map<String, Object> statsAfter = graphStore.getStats();
+        assertEquals(0L, statsAfter.get("node_count"));
+        assertEquals(0L, statsAfter.get("relationship_count"));
+    }
+
+    @Test
+    @DisplayName("Should provide accurate statistics")
+    void testGetStats() {
+        addTestRelationships();
+
+        Map<String, Object> stats = graphStore.getStats();
+
+        assertTrue((Long) stats.get("node_count") >= 3); // At least Alice, Bob, Charlie
+        assertTrue((Long) stats.get("relationship_count") >= 2); // At least 2 relationships
+        assertEquals("kuzu", stats.get("provider"));
+        assertTrue((Boolean) stats.get("embedded_mode"));
+        assertTrue((Boolean) stats.get("enabled"));
+        assertNotNull(stats.get("database_path"));
+    }
+
+    @Test
+    @DisplayName("Should handle errors gracefully")
+    void testErrorHandling() {
+        // 尝试删除不存在的关系
+        Map<String, Object> result = graphStore.deleteMemory("NonExistent", "Also", "NOTHING");
+
+        // 应该返回成功状态（即使关系不存在）
+        assertEquals("delete_graph_memory", result.get("action"));
+        assertEquals("success", result.get("status"));
+    }
+
+    @Test
+    @DisplayName("Should handle complex relationship patterns")
+    void testComplexRelationships() {
+        // 创建复杂的关系网络
+        graphStore.addMemory("Company", "Alice", "EMPLOYS", "ORGANIZATION", "PERSON");
+        graphStore.addMemory("Company", "Bob", "EMPLOYS", "ORGANIZATION", "PERSON");
+        graphStore.addMemory("Alice", "Project1", "WORKS_ON", "PERSON", "PROJECT");
+        graphStore.addMemory("Bob", "Project1", "WORKS_ON", "PERSON", "PROJECT");
+        graphStore.addMemory("Alice", "Bob", "COLLABORATES_WITH", "PERSON", "PERSON");
+
+        // 验证网络结构
+        List<Map<String, Object>> aliceRels = graphStore.getNodeRelationships("Alice");
+        assertTrue(aliceRels.size() >= 3);
+
+        List<Map<String, Object>> companyRels = graphStore.getNodeRelationships("Company");
+        assertTrue(companyRels.size() >= 2);
+
+        List<Map<String, Object>> project1Rels = graphStore.getNodeRelationships("Project1");
+        assertTrue(project1Rels.size() >= 2);
+    }
+
+    private void addTestRelationships() {
+        graphStore.addMemory("Alice", "Bob", "KNOWS", "PERSON", "PERSON");
+        graphStore.addMemory("Bob", "Charlie", "WORKS_WITH", "PERSON", "PERSON");
+        graphStore.addMemory("Alice", "Charlie", "MANAGES", "PERSON", "PERSON");
+    }
+}
\ No newline at end of file
diff --git a/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/LocalEmbeddedIntegrationTest.java b/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/LocalEmbeddedIntegrationTest.java
new file mode 100644
index 000000000..d3dcc12ba
--- /dev/null
+++ b/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/LocalEmbeddedIntegrationTest.java
@@ -0,0 +1,412 @@
+package run.mone.hive.memory.longterm;
+
+import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.AfterEach;
+import org.junit.jupiter.api.DisplayName;
+import static org.junit.jupiter.api.Assertions.*;
+
+import run.mone.hive.memory.longterm.config.*;
+import run.mone.hive.memory.longterm.core.Memory;
+import run.mone.hive.memory.longterm.vectorstore.VectorStoreFactory;
+import run.mone.hive.memory.longterm.graph.GraphStoreFactory;
+import run.mone.hive.memory.longterm.vectorstore.VectorStoreBase;
+import run.mone.hive.memory.longterm.graph.GraphStoreBase;
+import run.mone.hive.memory.longterm.model.MemoryItem;
+
+import java.util.*;
+import java.io.File;
+
+@DisplayName("Local Embedded Memory Integration Tests")
+public class LocalEmbeddedIntegrationTest {
+
+    private Memory memory;
+    private VectorStoreBase vectorStore;
+    private GraphStoreBase graphStore;
+    private String testBaseDir = "./data/test/integration";
+
+    @BeforeEach
+    void setUp() {
+        // 清理测试目录
+        cleanupTestDirectory();
+
+        // 配置本地嵌入式向量存储
+        VectorStoreConfig vectorConfig = VectorStoreConfig.builder()
+                .provider(VectorStoreConfig.Provider.CHROMA)
+                .collectionName("integration_test")
+                .host("localhost")
+                .path(testBaseDir + "/chroma")
+                .embeddingModelDims(384)
+                .build();
+
+        // 配置本地嵌入式图存储
+        GraphStoreConfig graphConfig = GraphStoreConfig.builder()
+                .provider(GraphStoreConfig.Provider.KUZU)
+                .url(testBaseDir + "/kuzu")
+                .enabled(true)
+                .build();
+
+        // 配置LLM
+        LlmConfig llmConfig = LlmConfig.builder()
+                .provider(LlmConfig.Provider.OPENAI)
+                .model("gpt-3.5-turbo")
+                .build();
+
+        // 配置嵌入器
+        EmbedderConfig embedderConfig = EmbedderConfig.builder()
+                .provider(EmbedderConfig.Provider.OPENAI)
+                .model("text-embedding-ada-002")
+                .build();
+
+        // 创建内存配置
+        MemoryConfig memoryConfig = MemoryConfig.builder()
+                .vectorStore(vectorConfig)
+                .graphStore(graphConfig)
+                .llm(llmConfig)
+                .embedder(embedderConfig)
+                .build();
+
+        // 单独创建存储实例用于测试
+        vectorStore = VectorStoreFactory.create(vectorConfig);
+        graphStore = GraphStoreFactory.create(graphConfig);
+
+        // 创建内存实例
+        memory = new Memory(memoryConfig);
+    }
+
+    @AfterEach
+    void tearDown() {
+        if (vectorStore != null) {
+            vectorStore.close();
+        }
+        if (graphStore != null) {
+            graphStore.close();
+        }
+        if (memory != null) {
+            memory.close();
+        }
+        cleanupTestDirectory();
+    }
+
+    private void cleanupTestDirectory() {
+        File testDir = new File(testBaseDir);
+        if (testDir.exists()) {
+            deleteDirectory(testDir);
+        }
+    }
+
+    private void deleteDirectory(File directory) {
+        File[] files = directory.listFiles();
+        if (files != null) {
+            for (File file : files) {
+                if (file.isDirectory()) {
+                    deleteDirectory(file);
+                } else {
+                    file.delete();
+                }
+            }
+        }
+        directory.delete();
+    }
+
+    @Test
+    @DisplayName("Should initialize both stores successfully")
+    void testInitialization() {
+        assertNotNull(vectorStore);
+        assertNotNull(graphStore);
+        assertNotNull(memory);
+
+        assertTrue(vectorStore.validateConnection());
+        assertTrue(graphStore.validateConnection());
+
+        // 验证向量存储统计
+        Map<String, Object> vectorStats = vectorStore.getStats();
+        assertEquals("chroma", vectorStats.get("provider"));
+        assertTrue((Boolean) vectorStats.get("embedded_mode"));
+
+        // 验证图存储统计
+        Map<String, Object> graphStats = graphStore.getStats();
+        assertEquals("kuzu", graphStats.get("provider"));
+        assertTrue((Boolean) graphStats.get("embedded_mode"));
+    }
+
+    @Test
+    @DisplayName("Should work together for vector and graph operations")
+    void testVectorAndGraphIntegration() {
+        // 1. 添加向量数据
+        List<List<Double>> vectors = Arrays.asList(
+            Arrays.asList(0.1, 0.2, 0.3, 0.4),
+            Arrays.asList(0.5, 0.6, 0.7, 0.8),
+            Arrays.asList(0.2, 0.3, 0.4, 0.5)
+        );
+
+        List<String> ids = Arrays.asList("memory1", "memory2", "memory3");
+
+        List<Map<String, Object>> payloads = Arrays.asList(
+            createMemoryPayload("Alice loves programming", "user1"),
+            createMemoryPayload("Bob works with Alice on projects", "user1"),
+            createMemoryPayload("Charlie manages the team", "user1")
+        );
+
+        vectorStore.insert(vectors, ids, payloads);
+
+        // 2. 添加图关系
+        graphStore.addMemory("Alice", "Programming", "LOVES", "PERSON", "ACTIVITY");
+        graphStore.addMemory("Bob", "Alice", "WORKS_WITH", "PERSON", "PERSON");
+        graphStore.addMemory("Alice", "Bob", "COLLABORATES", "PERSON", "PERSON");
+        graphStore.addMemory("Charlie", "Team", "MANAGES", "PERSON", "GROUP");
+
+        // 3. 验证向量存储
+        assertEquals(3L, vectorStore.getVectorCount());
+        List<MemoryItem> vectorResults = vectorStore.search("Alice",
+            Arrays.asList(0.15, 0.25, 0.35, 0.45), 5, null);
+        assertFalse(vectorResults.isEmpty());
+
+        // 4. 验证图存储
+        assertTrue(graphStore.relationshipExists("Alice", "Programming", "LOVES"));
+        assertTrue(graphStore.relationshipExists("Bob", "Alice", "WORKS_WITH"));
+
+        List<Map<String, Object>> aliceRelations = graphStore.getNodeRelationships("Alice");
+        assertTrue(aliceRelations.size() >= 2);
+
+        // 5. 验证组合查询能力
+        List<Map<String, Object>> graphSearchResults = graphStore.search("Alice", 10);
+        assertFalse(graphSearchResults.isEmpty());
+
+        Map<String, Object> vectorFilters = Map.of("user_id", "user1");
+        List<MemoryItem> filteredVectorResults = vectorStore.search("Bob",
+            Arrays.asList(0.5, 0.6, 0.7, 0.8), 5, vectorFilters);
+        assertFalse(filteredVectorResults.isEmpty());
+    }
+
+    @Test
+    @DisplayName("Should handle complex knowledge graph scenarios")
+    void testComplexKnowledgeGraph() {
+        // 创建复杂的知识图谱
+        // 公司结构
+        graphStore.addMemory("TechCorp", "Alice", "EMPLOYS", "COMPANY", "PERSON");
+        graphStore.addMemory("TechCorp", "Bob", "EMPLOYS", "COMPANY", "PERSON");
+        graphStore.addMemory("TechCorp", "Charlie", "EMPLOYS", "COMPANY", "PERSON");
+
+        // 团队关系
+        graphStore.addMemory("Alice", "DevTeam", "LEADS", "PERSON", "TEAM");
+        graphStore.addMemory("Bob", "DevTeam", "MEMBER_OF", "PERSON", "TEAM");
+        graphStore.addMemory("Charlie", "QATeam", "LEADS", "PERSON", "TEAM");
+
+        // 项目关系
+        graphStore.addMemory("DevTeam", "ProjectX", "WORKS_ON", "TEAM", "PROJECT");
+        graphStore.addMemory("QATeam", "ProjectX", "TESTS", "TEAM", "PROJECT");
+
+        // 技能关系
+        graphStore.addMemory("Alice", "Java", "SKILLED_IN", "PERSON", "SKILL");
+        graphStore.addMemory("Alice", "Python", "SKILLED_IN", "PERSON", "SKILL");
+        graphStore.addMemory("Bob", "JavaScript", "SKILLED_IN", "PERSON", "SKILL");
+
+        // 验证复杂查询
+        List<Map<String, Object>> aliceConnections = graphStore.getNodeRelationships("Alice");
+        assertTrue(aliceConnections.size() >= 4); // EMPLOYS + LEADS + 2 SKILLED_IN
+
+        List<Map<String, Object>> projectXRelated = graphStore.search("ProjectX", 10);
+        assertTrue(projectXRelated.size() >= 2); // WORKS_ON + TESTS
+
+        List<Map<String, Object>> techCorpEmployees = graphStore.search("TechCorp", 10);
+        assertTrue(techCorpEmployees.size() >= 3); // 3 EMPLOYS relationships
+    }
+
+    @Test
+    @DisplayName("Should support real-world memory scenarios")
+    void testRealWorldMemoryScenarios() {
+        // 场景1: 用户对话记忆
+        String[] conversations = {
+            "I love playing guitar in my free time",
+            "I work as a software engineer at Google",
+            "My favorite programming language is Python",
+            "I have a cat named Whiskers",
+            "I'm planning to visit Japan next year"
+        };
+
+        // 为每个对话创建向量和图关系
+        for (int i = 0; i < conversations.length; i++) {
+            String conversation = conversations[i];
+            String memoryId = "conv_" + i;
+
+            // 添加向量记忆
+            List<Double> fakeEmbedding = generateFakeEmbedding(conversation);
+            vectorStore.insert(
+                Arrays.asList(fakeEmbedding),
+                Arrays.asList(memoryId),
+                Arrays.asList(createMemoryPayload(conversation, "user123"))
+            );
+
+            // 提取并添加图关系
+            addConversationRelations(conversation);
+        }
+
+        // 验证记忆存储
+        assertEquals(5L, vectorStore.getVectorCount());
+
+        // 搜索相关记忆
+        List<MemoryItem> hobbyMemories = vectorStore.search("hobby",
+            generateFakeEmbedding("hobby"), 3, null);
+        assertFalse(hobbyMemories.isEmpty());
+
+        List<MemoryItem> workMemories = vectorStore.search("work",
+            generateFakeEmbedding("work"), 3, null);
+        assertFalse(workMemories.isEmpty());
+
+        // 验证图关系
+        assertTrue(graphStore.relationshipExists("User", "Guitar", "PLAYS"));
+        assertTrue(graphStore.relationshipExists("User", "Google", "WORKS_AT"));
+        assertTrue(graphStore.relationshipExists("User", "Python", "PREFERS"));
+
+        List<Map<String, Object>> userRelations = graphStore.getNodeRelationships("User");
+        assertTrue(userRelations.size() >= 5);
+    }
+
+    @Test
+    @DisplayName("Should maintain data persistence across restarts")
+    void testDataPersistence() {
+        // 添加初始数据
+        vectorStore.insert(
+            Arrays.asList(Arrays.asList(0.1, 0.2, 0.3, 0.4)),
+            Arrays.asList("persistent_memory"),
+            Arrays.asList(createMemoryPayload("This should persist", "user1"))
+        );
+
+        graphStore.addMemory("DataA", "DataB", "CONNECTS_TO", "DATA", "DATA");
+
+        // 验证数据存在
+        MemoryItem item = vectorStore.get("persistent_memory");
+        assertNotNull(item);
+        assertTrue(graphStore.relationshipExists("DataA", "DataB", "CONNECTS_TO"));
+
+        // 模拟重启 - 关闭连接
+        vectorStore.close();
+        graphStore.close();
+
+        // 重新初始化
+        VectorStoreConfig vectorConfig = VectorStoreConfig.builder()
+                .provider(VectorStoreConfig.Provider.CHROMA)
+                .collectionName("integration_test")
+                .host("localhost")
+                .path(testBaseDir + "/chroma")
+                .embeddingModelDims(384)
+                .build();
+
+        GraphStoreConfig graphConfig = GraphStoreConfig.builder()
+                .provider(GraphStoreConfig.Provider.KUZU)
+                .url(testBaseDir + "/kuzu")
+                .enabled(true)
+                .build();
+
+        vectorStore = VectorStoreFactory.create(vectorConfig);
+        graphStore = GraphStoreFactory.create(graphConfig);
+
+        // 验证数据仍然存在
+        MemoryItem persistedItem = vectorStore.get("persistent_memory");
+        assertNotNull(persistedItem);
+        assertEquals("This should persist", persistedItem.getMemory());
+
+        assertTrue(graphStore.relationshipExists("DataA", "DataB", "CONNECTS_TO"));
+    }
+
+    @Test
+    @DisplayName("Should handle concurrent operations safely")
+    void testConcurrentOperations() {
+        // 这是一个简化的并发测试
+        // 在真实环境中，你可能需要更复杂的并发测试
+
+        // 同时执行多个操作
+        List<Thread> threads = new ArrayList<>();
+
+        // 向量插入线程
+        for (int i = 0; i < 3; i++) {
+            final int threadId = i;
+            threads.add(new Thread(() -> {
+                for (int j = 0; j < 5; j++) {
+                    String id = "thread" + threadId + "_mem" + j;
+                    vectorStore.insert(
+                        Arrays.asList(generateFakeEmbedding("test " + threadId + " " + j)),
+                        Arrays.asList(id),
+                        Arrays.asList(createMemoryPayload("Memory from thread " + threadId, "user" + threadId))
+                    );
+                }
+            }));
+        }
+
+        // 图关系添加线程
+        for (int i = 0; i < 2; i++) {
+            final int threadId = i;
+            threads.add(new Thread(() -> {
+                for (int j = 0; j < 3; j++) {
+                    graphStore.addMemory(
+                        "Entity" + threadId + "_" + j,
+                        "Target" + threadId + "_" + j,
+                        "RELATES_TO",
+                        "TYPE",
+                        "TYPE"
+                    );
+                }
+            }));
+        }
+
+        // 启动所有线程
+        threads.forEach(Thread::start);
+
+        // 等待所有线程完成
+        threads.forEach(thread -> {
+            try {
+                thread.join();
+            } catch (InterruptedException e) {
+                Thread.currentThread().interrupt();
+            }
+        });
+
+        // 验证所有数据都已正确插入
+        assertTrue(vectorStore.getVectorCount() >= 15); // 3 threads * 5 memories
+
+        Map<String, Object> graphStats = graphStore.getStats();
+        assertTrue((Long) graphStats.get("relationship_count") >= 6); // 2 threads * 3 relationships
+    }
+
+    // 辅助方法
+    private Map<String, Object> createMemoryPayload(String memory, String userId) {
+        Map<String, Object> payload = new HashMap<>();
+        payload.put("memory", memory);
+        payload.put("user_id", userId);
+        payload.put("agent_id", "test_agent");
+        payload.put("run_id", "integration_test_run");
+        payload.put("role", "user");
+        return payload;
+    }
+
+    private List<Double> generateFakeEmbedding(String text) {
+        // 生成基于文本的伪嵌入向量
+        Random random = new Random(text.hashCode());
+        List<Double> embedding = new ArrayList<>();
+        for (int i = 0; i < 4; i++) {
+            embedding.add(random.nextDouble());
+        }
+        return embedding;
+    }
+
+    private void addConversationRelations(String conversation) {
+        String lowerConv = conversation.toLowerCase();
+
+        if (lowerConv.contains("guitar")) {
+            graphStore.addMemory("User", "Guitar", "PLAYS", "PERSON", "INSTRUMENT");
+        }
+        if (lowerConv.contains("google")) {
+            graphStore.addMemory("User", "Google", "WORKS_AT", "PERSON", "COMPANY");
+        }
+        if (lowerConv.contains("python")) {
+            graphStore.addMemory("User", "Python", "PREFERS", "PERSON", "LANGUAGE");
+        }
+        if (lowerConv.contains("cat")) {
+            graphStore.addMemory("User", "Whiskers", "OWNS", "PERSON", "PET");
+        }
+        if (lowerConv.contains("japan")) {
+            graphStore.addMemory("User", "Japan", "PLANS_TO_VISIT", "PERSON", "COUNTRY");
+        }
+    }
+}
\ No newline at end of file
diff --git a/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/LocalMemoryIntegrationTest.java b/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/LocalMemoryIntegrationTest.java
index 076e0a329..a98aed476 100644
--- a/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/LocalMemoryIntegrationTest.java
+++ b/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/LocalMemoryIntegrationTest.java
@@ -6,6 +6,7 @@ import org.junit.jupiter.api.io.TempDir;
 
 import run.mone.hive.memory.longterm.config.*;
 import run.mone.hive.memory.longterm.core.Memory;
+import run.mone.hive.memory.longterm.vectorstore.impl.ChromaVectorStore;
 
 import java.nio.file.Path;
 import java.util.*;
@@ -42,23 +43,31 @@ public class LocalMemoryIntegrationTest {
         MemoryConfig config = MemoryConfig.builder()
             .llm(LlmConfig.builder()
                 .provider(LlmConfig.Provider.OPENAI)
-                .model("gpt-4o-mini")
-                .apiKey("test-key")  // 测试时不会真正调用
+                .model("gpt-4o")
+                .baseUrl("")
+                .apiKey("")  // 测试时不会真正调用
+                .customHeaders(Map.of("X-Model-Provider-Id", "azure_openai"))
                 .build())
             .embedder(EmbedderConfig.builder()
                 .provider(EmbedderConfig.Provider.OPENAI)
                 .model("text-embedding-3-small")
-                .apiKey("test-key")  // 测试时不会真正调用
+                .baseUrl("")
+                .apiKey("")  // 测试时不会真正调用
+                .customHeaders(Map.of("X-Model-Provider-Id", "azure_openai"))
                 .build())
             .vectorStore(VectorStoreConfig.builder()
-                .provider(VectorStoreConfig.Provider.LOCAL)
+                .provider(VectorStoreConfig.Provider.CHROMA)
                 .collectionName("test_collection")
                 .path(tempDir.resolve("vector").toString())
                 .embeddingModelDims(1536)
+                .embeddingFunction(ChromaVectorStore.OPENAI_EMBEDDING_FUNCTION)
+                .apiKey("")
+                .baseUrl("")
                 .build())
             .graphStore(GraphStoreConfig.builder()
-                .provider(GraphStoreConfig.Provider.LOCAL)
-                .url(tempDir.resolve("graph").toString())
+                .provider(GraphStoreConfig.Provider.KUZU)
+                // .url(tempDir.resolve("graph").toString())
+                // .url(":memory:")
                 .enabled(true)
                 .build())
             .historyDbPath(tempDir.resolve("history.db").toString())
diff --git a/jcommon/hive/src/main/java/run/mone/hive/memory/longterm/QuickVerificationDemo.java b/jcommon/hive/src/test/java/run/mone/hive/memory/longterm/QuickVerificationDemo.java
similarity index 100%
rename from jcommon/hive/src/main/java/run/mone/hive/memory/longterm/QuickVerificationDemo.java
rename to jcommon/hive/src/test/java/run/mone/hive/memory/longterm/QuickVerificationDemo.java
